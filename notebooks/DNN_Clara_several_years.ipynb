{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 09 14:36 2021\n",
    "\n",
    "Prepare proof of concept with a very simple DNN to parameterise the sub-shelf melt - more advanced than 1st try\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_run = 'OPM006'\n",
    "inputpath_data='/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/NEMO_eORCA025.L121_'+nemo_run+'_ANT_STEREO/'\n",
    "inputpath_mask = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_profiles = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/T_S_PROF/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_plumes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/PLUMES/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_boxes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/BOXES/nemo_5km_'+nemo_run+'/'\n",
    "outputpath_melt = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "outputpath_nn = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/data/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR EACH POINT:\n",
    "- T and S profiles at the front (decompose z dimension into single things)\n",
    "- Distance to front\n",
    "- Distance to the grounding line\n",
    "- Local slope ice draft\n",
    "- Local slope bedrock\n",
    "- Ice draft depth\n",
    "- Bathymetry\n",
    "- Ice draft concentration\n",
    "- Horizontal coordinates (lon, lat)\n",
    "- Mean bathymetry at entry (to add in future)\n",
    "- Max bathymetry (to add in future)\n",
    "- Target: melt m ice per yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dIF, dGL, longitude, latitude\n",
    "file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T and S profiles\n",
    "file_TS_orig = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_and_offshore_1980-2018.nc')\n",
    "file_TS = file_TS_orig.sel(Nisf=file_isf.Nisf)\n",
    "file_TS_dom = file_TS.sel(profile_domain=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_charac = xr.open_dataset(inputpath_plumes+'nemo_5km_plume_characteristics.nc')\n",
    "# Local slope\n",
    "local_ice_slope = plume_charac['alpha'].sel(option='appenB').drop('option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]\n",
    "file_mask_orig = xr.open_dataset(inputpath_data+'other_mask_vars_Ant_stereo.nc')\n",
    "file_mask_orig_cut = dfmt.cut_domain_stereo(file_mask_orig, map_lim, map_lim)\n",
    "file_other = xr.open_dataset(inputpath_data+'corrected_draft_bathy_isf.nc')#, chunks={'x': chunk_size, 'y': chunk_size})\n",
    "file_other_cut = dfmt.cut_domain_stereo(file_other, map_lim, map_lim)\n",
    "file_conc = xr.open_dataset(inputpath_data+'isfdraft_conc_Ant_stereo.nc')\n",
    "file_conc_cut = dfmt.cut_domain_stereo(file_conc, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathymetry, ice draft, concentration\n",
    "file_bed_orig = file_mask_orig_cut['bathy_metry']\n",
    "file_draft = file_other_cut['corrected_isfdraft'] \n",
    "file_isf_conc = file_conc_cut['isfdraft_conc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bedrock_slope = xr.open_dataset(inputpath_mask+'nemo_5km_bedrock_slope.nc')\n",
    "local_bedrock_slope = file_bedrock_slope['bedrock_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEMO_melt_rates_2D = xr.open_mfdataset(outputpath_melt+'melt_rates_2D_NEMO.nc')\n",
    "melt_rate = NEMO_melt_rates_2D['melt_m_ice_per_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all 2D data in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left out 'longitude', 'latitude'\n",
    "geometry_2D = file_isf[['dGL', 'dIF']].merge(local_ice_slope).merge(local_bedrock_slope).merge(file_draft).merge(file_bed_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBSAMPLE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one ice shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisf_of_int = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D_isf = geometry_2D.where(file_isf['ISF_mask'] == kisf_of_int, drop=True)\n",
    "melt_rate_isf = melt_rate.where(file_isf['ISF_mask'] == kisf_of_int, drop=True).load()\n",
    "TS_isf = file_TS_dom.sel(Nisf=kisf_of_int)\n",
    "max_front_depth = file_isf['front_bot_depth_max'].sel(Nisf=kisf_of_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATAFRAME WITH ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_yy_df = None\n",
    "for kk, yyyy in enumerate(tqdm(range(1980,2010))):\n",
    "    clean_df_yy, T_list, S_list = dfmt.prepare_input_df_1_year(TS_isf, melt_rate_isf, yyyy, max_front_depth, geometry_2D_isf)\n",
    "    if kk > 0:\n",
    "        merged_yy_df = merged_yy_df.append(clean_df_yy, ignore_index = True)\n",
    "    else:\n",
    "        merged_yy_df = clean_df_yy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_yy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = merged_yy_df.drop(['time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(outputpath_nn + 'input_data_1980-2010_isf'+str(kisf_of_int).zfill(3)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_files = glob.glob(outputpath_nn + 'input_data_1980-2010_isf*.csv')\n",
    "#clean_df = pd.concat((pd.read_csv(f) for f in all_files)).drop('Unnamed: 0', 1)\n",
    "#clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only one ice shelf\n",
    "clean_df = pd.read_csv(outputpath_nn + 'input_data_1980-2010_isf'+str(kisf_of_int).zfill(3)+'.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_list_drop = [ ]\n",
    "S_list_drop = [ ]\n",
    "for ii in range(60,85):\n",
    "    T_list_drop.append('T_'+str(ii).zfill(3))\n",
    "    S_list_drop.append('S_'+str(ii).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_list = [ ]\n",
    "S_list = [ ]\n",
    "for ii in range(60):\n",
    "    T_list.append('T_'+str(ii).zfill(3))\n",
    "    S_list.append('S_'+str(ii).zfill(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(T_list_drop, 1)\n",
    "clean_df = clean_df.drop(S_list_drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop('melt_cavity', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = clean_df.sample(frac=0.7, axis=0) \n",
    "data_test  = clean_df.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_m_ice_per_y']\n",
    "x_train = data_train.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "y_test = data_test['melt_m_ice_per_y']\n",
    "x_test = data_test.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "print('Original data shape was : ',clean_df.shape)\n",
    "print('x_train : ',x_train.shape, 'y_train : ',y_train.shape)\n",
    "print('x_test  : ',x_test.shape,  'y_test  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Data normalization\n",
    "**Note :** \n",
    " - All input data must be normalized, train and test.  \n",
    " - To do this we will **subtract the mean** and **divide by the standard deviation**.  \n",
    " - But test data should not be used in any way, even for normalization.  \n",
    " - The mean and the standard deviation will therefore only be calculated with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n",
    "\n",
    "x_train_norm = x_train.copy()\n",
    "x_test_norm = x_test.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude']:\n",
    "    mean = x_train[ccol].mean()\n",
    "    std  = x_train[ccol].std()\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean) / std\n",
    "    x_test_norm[ccol]  = (x_test[ccol]  - mean) / std\n",
    "\n",
    "mean_T = x_train[T_list].mean().mean()\n",
    "std_T = x_train[T_list].mean().std()\n",
    "mean_S = x_train[S_list].mean().mean()\n",
    "std_S = x_train[S_list].mean().std()\n",
    "\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean_T) / std_T\n",
    "    x_test_norm[ccol] = (x_test[ccol] - mean_T) / std_T\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean_S) / std_S\n",
    "    x_test_norm[ccol] = (x_test[ccol] - mean_S) / std_S\n",
    "\n",
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "#display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "\n",
    "x_train_arr, y_train_arr = np.array(x_train_norm), np.array(y_train)\n",
    "x_test_arr,  y_test_arr  = np.array(x_test_norm),  np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_v2(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "#def get_model_v1(shape):\n",
    "#    nodes = 256\n",
    "#    #activ = 'sigmoid'\n",
    "#   activ = 'relu'   # standard\n",
    "#    #activ = 'tanh'\n",
    "#    #activ = 'selu'\n",
    "#    model = keras.models.Sequential()\n",
    "#    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n1'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n2'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n3'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n4'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n5'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n6'))\n",
    "#    model.add(keras.layers.Dense(1, name='Output'))  \n",
    "#                                    # nbvect = number of elements of target vector\n",
    "#\n",
    "#    model.compile(optimizer = 'rmsprop',\n",
    "#                  loss      = 'mse',                  # mse, mean quadratic error \n",
    "#              metrics   = ['mae', 'mse'] )        # mae =  mean absolute error\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x_train_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_v1( (input_size,) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_arr,\n",
    "                    y_train_arr,\n",
    "                    epochs          = 40,\n",
    "                    batch_size      = 10,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_arr, y_test_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_arr, y_test_arr, verbose=1)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, yyyy in enumerate(tqdm(range(2018,2019))):\n",
    "    clean_df_yy_val, T_list, S_list = dfmt.prepare_input_df_1_year(TS_isf, melt_rate_isf, yyyy, max_front_depth, geometry_2D_isf)\n",
    "    if kk > 0:\n",
    "        merged_yy_df_val = merged_yy_df_val.append(clean_df_yy_val, ignore_index = True)\n",
    "    else:\n",
    "        merged_yy_df_val = clean_df_yy_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_val = merged_yy_df_val.drop(['time'], axis=1).reset_index().drop(['x'], axis=1).drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df_val = clean_df_val.drop(T_list_drop, 1)\n",
    "#clean_df_val = clean_df_val.drop(S_list_drop, 1)\n",
    "#clean_df_val = clean_df_val.drop('melt_cavity', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = clean_df_val['melt_m_ice_per_y']\n",
    "x_val = clean_df_val.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "x_val_norm = x_val.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude']:\n",
    "    mean = x_train[ccol].mean()\n",
    "    std  = x_train[ccol].std()\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean) / std\n",
    "\n",
    "mean_T = x_train[T_list].mean().mean()\n",
    "std_T = x_train[T_list].mean().std()\n",
    "mean_S = x_train[S_list].mean().mean()\n",
    "std_S = x_train[S_list].mean().std()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean_T) / std_T\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean_S) / std_S\n",
    "\n",
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "#display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "\n",
    "x_val_arr, y_val_arr = np.array(x_val_norm), np.array(y_val)\n",
    "\n",
    "#my_data=np.array(x_val_arr)#.reshape(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(x_val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_pd_s = pd.Series(y_out[:,0],index=merged_yy_df_val.index,name='computed_melt') \n",
    "y_target_pd_s = pd.Series(y_val_arr,index=merged_yy_df_val.index,name='reference_melt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_xr = y_out_pd_s.to_xarray()\n",
    "y_target_xr = y_target_pd_s.to_xarray()\n",
    "y_to_compare = xr.merge([y_out_xr.T, y_target_xr.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(0,80)\n",
    "plt.figure()\n",
    "plt.scatter(y_to_compare['computed_melt'].values.flatten(),y_to_compare['reference_melt'].values.flatten(), s=10, edgecolors='None',alpha=0.2)\n",
    "plt.plot(xx,xx,'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_melt = y_to_compare['computed_melt']\n",
    "ref_melt = y_to_compare['reference_melt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_m = min(computed_melt.min(), ref_melt.min())\n",
    "max_m = max(computed_melt.max(), ref_melt.max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "computed_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax1.set_title('Neural Network [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "ref_melt.plot(ax=ax2, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax2.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(computed_melt - ref_melt).plot(ax=ax3)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('NN - Ref [m ice/y]')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_compare.to_netcdf(outputpath_nn+'prediction_linreg_2018_withoutlatlon.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_compare_nn = xr.open_dataset(outputpath_nn+'prediction_nn_2018.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_to_compare_nn - y_to_compare)['computed_melt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((y_out - y_val_arr)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )\n",
    "print(\"Prediction : {:.2f} m ice per y\".format(predictions[0][0]))\n",
    "print(\"Reality    : {:.2f} m ice per y\".format(y_val_arr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARE WITH SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_param = xr.open_dataset(outputpath_melt+'melt_rates_2D_quadratic_mixed_locslope_tuned_correctedTS.nc')\n",
    "melt_simple = simple_param['melt_m_ice_per_y'].sel(profile_domain=50,time=2018).where(file_isf['ISF_mask']==66, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_compare_nn = xr.open_dataset(outputpath_nn+'prediction_linreg_2018_withoutlatlon.nc')\n",
    "computed_melt = y_to_compare_nn['computed_melt']\n",
    "ref_melt = y_to_compare_nn['reference_melt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/bettik/burgardc/PLOTS/generic_plots/'\n",
    "min_m = min(computed_melt.min(), ref_melt.min())\n",
    "max_m = max(computed_melt.max(), ref_melt.max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "    cmap_diff = mpl.cm.BrBG_r\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "    cmap_diff = mpl.cm.BuGn\n",
    "    \n",
    "f = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ref_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax1.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=minlim,vmax=lim, cmap=cmap_diff)\n",
    "ax2.set_title('Neural Network param - Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(melt_simple - ref_melt).plot(ax=ax3, vmin=minlim,vmax=lim, cmap=cmap_diff)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('Simple physical param - Reference [m ice/y]')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(plot_path+'comparison_proof_of_concept.png', dpi=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plot_path = '/bettik/burgardc/PLOTS/generic_plots/'\n",
    "min_m = min(computed_melt.min(), ref_melt.min())\n",
    "max_m = max(computed_melt.max(), ref_melt.max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8.24*1.3, 8.24/3)\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ref_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=mpl.cm.coolwarm)\n",
    "ax1.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "ax2.set_title('Neural Network param - Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(melt_simple - ref_melt).plot(ax=ax3, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('Simple physical param - Reference [m ice/y]')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(plot_path+'comparison_proof_of_concept_for_ANR_proposal.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/bettik/burgardc/PLOTS/generic_plots/'\n",
    "min_m = min((computed_melt - ref_melt).min(), ref_melt.min())\n",
    "max_m = max((computed_melt - ref_melt).max(), ref_melt.max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "    cmap_diff = mpl.cm.BrBG_r\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "    cmap_diff = mpl.cm.copper\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8.24*1.3, 8.24/3)\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ref_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax1.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=minlim,vmax=lim, cmap=cmap_diff)\n",
    "ax2.set_title('Neural Network param - Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(melt_simple - ref_melt).plot(ax=ax3, vmin=minlim,vmax=lim, cmap=cmap_diff)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('Simple physical param - Reference [m ice/y]')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(plot_path+'comparison_proof_of_concept_for_ANR_proposal_withoutlatlon.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.axes(projection=ccrs.SouthPolarStereo(central_longitude=0,true_scale_latitude=-71))\n",
    "#ax.coastlines(resolution='50m', linewidth=0.5)\n",
    "#ax.pcolormesh(melt_simple.longitude,melt_simple.latitude,melt_simple,transform=ccrs.PlateCarree(),rasterized=True)\n",
    "#ax.set_extent([-180, 180, -90, -60], crs=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== TO KEEP FOR THE FUTURE ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column - for normalization with min and max\n",
    "\n",
    "normalized_clean_df = clean_df.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude','melt_cavity','time']:\n",
    "    max_ccol = clean_df[ccol].max()\n",
    "    min_ccol = clean_df[ccol].min()\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_ccol)/(max_ccol - min_ccol)\n",
    "\n",
    "max_T = clean_df[T_list].max().max()\n",
    "min_T = clean_df[T_list].min().min()\n",
    "max_S = clean_df[S_list].max().max()\n",
    "min_S = clean_df[S_list].min().min()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_T)/(max_T - min_T)\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_S)/(max_S - min_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
