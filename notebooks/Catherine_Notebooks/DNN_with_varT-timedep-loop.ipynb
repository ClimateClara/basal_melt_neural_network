{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <!-- TITLE --> [GRISLI temperature] \n",
    "## Regression with a Dense Neural Network (DNN)  \n",
    "\n",
    "\n",
    "- AUTHOR : catritz \n",
    "- date 18 janvier2021\n",
    "- reprise 13 février 2021\n",
    "- reprise (pour EGU) le 21 Avril 2021\n",
    "\n",
    "## version with time dependent surface temperature as input\n",
    "- on **difference with surface temperature**\n",
    "-  using other predictors such as **varTb**\n",
    "- reading data from xarray\n",
    "\n",
    "\n",
    "## Objectives :\n",
    " - Predicts **temperature profile** from a set of parameters\n",
    " - with a **dense neural network**  \n",
    "\n",
    "\n",
    "### Database : \n",
    "- generated with read-GRISLI-xarray.ipynb\n",
    "- read-GRISLI-xarray-time-dep.ipynb\n",
    "\n",
    "- data in ./data/Grisli_4_DNN_AN40C008.nc and runs 8-11 \n",
    "- and in ./data/Grisli_4_DNN_time_dep_AN40C008.nc runs 8-11\n",
    "\n",
    "- variables\n",
    "\n",
    "    * H: ice thickness (m)\n",
    "    * ghf (in mw/m2)\n",
    "    * Ts:surface temperature (°C)\n",
    "    * acc: accumulation rate (m/yr)\n",
    "    * base: basal temperature (difference from the melting point)    \n",
    "    \n",
    "- time dependent surface temperature\n",
    "    * Tann over the last 150 k, take only the last 25 k\n",
    "    \n",
    " \n",
    " **The vector of temperature profile**\n",
    " - T_00,T_01,T_02,T_03,T_04,T_05,T_06,T_07,T_08,T_09,T_10,T_11,T_12,T_13,T_14,T_15,T_16,T_17,T_18,T_19,T_20\n",
    " \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What we're going to do :\n",
    "\n",
    " \n",
    "### 1. define title and parameters of the run \n",
    "- 1.1 define all parameters\n",
    "- 1.2 store parameters in a dictionnary <span style='color:blue '> dictionnary **docDNN** </span>   \n",
    "     \n",
    "### 2. Import the data to build the model \n",
    "- Retrieve data in netcdf as a xarray dataset  \n",
    " > pd_onefile = nc2pd(file_in) \n",
    "- loop on several netcdf\n",
    "- final <span style='color:blue '> dataframe **df_tot** </span>\n",
    "\n",
    "### 3. Prepare the data\n",
    "\n",
    "According to what has been defined in the parameter list\n",
    "\n",
    "- 3.1 remove floating part, thickness too small and eventually other caracteristics\n",
    "> data2 = select_some(df_tot,docDNN)\n",
    "- 3.2 to work in difference with surface temperature\n",
    "> data2 = diff_surf(data2,docDNN )\n",
    "- 3.3 split the data between train and test\n",
    "<span style='color:blue '> **data_train and data_test** </span>\n",
    "- 3.4 split x (predictor) and y (target) : <span style='color:blue '> dataframes **x_train, x_test, y_train, y_test** </span>\n",
    "- 3.5 - Data normalization\n",
    "    * 3.5.1 normalization itself ->  <span style='color:blue '> dataframes **x_trainnorm, x_testnorm** </span>\n",
    "    - to use the normalisation\n",
    "    > newx = get_norm(x,dic)    \n",
    "    > where x is a dataframe\n",
    "    \n",
    "    * 3.5.2 Save the characteristics of the run in .json file including normalization: mean_list, std_list\n",
    "- 3.6 back to <span style='color:blue '> numpy arrays : **x_train2, x_test2, y_train2 , y_test2** </span> (one keep dimension values in numpy arrays: x_traintrue, x_testrue)\n",
    " \n",
    "## 4. Build a model\n",
    "\n",
    "## 5. - Train the model\n",
    "- 5.1  Get the model and show a display of the model\n",
    "- 5.2  Define model check points \n",
    "- 5.3  Train \n",
    "\n",
    "## 6.  Evaluate\n",
    "- 6.1 Model evaluation\n",
    "- 6.2 training history \n",
    "- 6.3 Retrieve the best model obtained\n",
    "\n",
    "## 7. Predict\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    "## lectures\n",
    "- geometrie du modèle\n",
    "https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "- loss\n",
    " https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    " \n",
    "## to import my own modules\n",
    "https://python.sdv.univ-paris-diderot.fr/14_creation_modules/\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from keras.layers import Dropout\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import cm\n",
    "\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import xarray as xr\n",
    "import math\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('..')\n",
    "import fidle.pwk as ooo\n",
    "\n",
    "ooo.init()\n",
    "os.makedirs('./run/',   mode=0o750, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilitary routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretty print of a dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(d, indent = 1):   # pretty print d'un dictionnaire\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subroutine nc2pdtd : read netcdf files and construct a dataframe with time dependent Tann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc2pdtd(file_in):     ### step 2. read a netcdf file from GRISLI obtained with : read-GRISLI-xarray.ipynb\n",
    "                           ### time dependent version\n",
    "\n",
    "    DD = xr.open_dataset(file_in)  #\n",
    "\n",
    "    \n",
    "### 2D variables at present time\n",
    "\n",
    "    Dss = DD.sel(z=0).drop(\"T3D\")   # drop la variable 3D du xarray dataset\n",
    "\n",
    "    pd_2D = Dss.drop(\"z\").to_dataframe()  # drop z and convert to dataframe\n",
    "    \n",
    "\n",
    " ### time dependent surface temperature in x_array D_td\n",
    "\n",
    "    # derive tjan-fev-2021-hdf5-jsonhe name of the time dependent file from file_in\n",
    "    numtime = 25   # numer of time slices taken into account\n",
    "    s='DNN_'\n",
    "    pos = file_in.rfind(s)\n",
    "    file_td = file_in[0:pos+len(s)]+'time_dep_'+ file_in[pos+len(s):]\n",
    "\n",
    "    # read the dataset\n",
    "    D_Td = xr.open_dataset(file_td)\n",
    "    print(D_Td.dims)\n",
    "\n",
    "    # keep only Tann and select a time subset\n",
    "#    Tann = D_Td.Tann.sel(time=slice(-numtime*1000,0)) # keep only Tann on numtime kyears\n",
    "    timelist = [-x*1000 for x in range(numtime+1)] \n",
    "\n",
    "    Tann = D_Td.Tann.sel(time = timelist) # keep only Tann on numtime kyears\n",
    "\n",
    "    # Convert in a dataframe in which each time slice will be a column\n",
    "    Time_cols = [\"Tann_\"+ str(x).zfill(2)+\"kyr\" for x in range(numtime+1)]  # list columns\n",
    "    #print (Time_cols)\n",
    "\n",
    "    dTann = Tann.to_dataframe()\n",
    "    dTann = dTann.unstack(level='time').swaplevel().sort_index() # move the time in columns\n",
    "                                                                 # at this index level \"time\" still \n",
    "                                                                 # appears as a sign of multiindex\n",
    "    \n",
    "    dTann.columns = Time_cols                                   # remove the index level \"time\"\n",
    "    \n",
    "    \n",
    "    pd_2Dtd = pd.merge(pd_2D,dTann,how='left',on=['x','y'])   \n",
    "    \n",
    "### 3D variablescharacteristics\n",
    "\n",
    "    dft = DD.T3D.to_dataframe()\n",
    "\n",
    "    dT3D = dft.unstack(level='z').swaplevel().sort_index()\n",
    "\n",
    "    # change the column names to get rid of level 'z' in the dataframe\n",
    "    dT3D.columns = (['T_00','T_01','T_02','T_03','T_04','T_05',\n",
    "                     'T_06','T_07','T_08','T_09','T_10',\n",
    "                    'T_11','T_12','T_13','T_14','T_15',\n",
    "                     'T_16','T_17','T_18','T_19','T_20'])\n",
    "    \n",
    "       \n",
    "\n",
    "    pd_onefile = pd.merge(pd_2Dtd,dT3D,how='left',on=['x','y'])\n",
    "    pd_onefile.reset_index(level=[0,1], inplace=True)      # transform indexes x and y to columns\n",
    "    \n",
    "    \n",
    "\n",
    "# extract the name of the GRISLI run and fill a column with it to enable sorting a specific case later.\n",
    "    posdeb = file_in.rfind('/')\n",
    "    posend = file_in.rfind('.nc')\n",
    "    subfile = file_in[posdeb+1:posend]    \n",
    "    pd_onefile['GRISLI_run'] = subfile    \n",
    " \n",
    "    \n",
    "    return pd_onefile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subroutine select_some : select points according to criteria in the dictionnary dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_some(df_tot,dic):   # select points according to criteria in dictionnary. section 3.1\n",
    "# return the dataframe data2    \n",
    "   \n",
    "    Hmin = dic['select_data']['thickmin'] \n",
    "    base_type = dic['select_data']['base']\n",
    "    mk = dic['select_data']['mk']\n",
    "    DTb = dic['select_data']['varTb'] \n",
    "\n",
    "    datagrounded = df_tot[df_tot.Mk < 0.5] \n",
    "\n",
    "\n",
    "    if DTb == 0:\n",
    "        datagrounded = datagrounded[datagrounded.VarTb == 0] \n",
    "\n",
    "\n",
    "    data2 =  datagrounded[datagrounded.H > Hmin] \n",
    "\n",
    "    if base_type == 'cold':\n",
    "        data2 =  data2[data2.Tb < 0] \n",
    "\n",
    "    elif base_type == 'wet':\n",
    "        data2 =  data2[data2.Tb == 0] \n",
    "\n",
    "    else: \n",
    "        data2 =  data2\n",
    "\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subroutine to make the difference with the surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_surf(df_in, dic):  # make the difference with surface temperature step 3.2\n",
    "    list_y = dic['y_columns']['list_y']\n",
    "    \n",
    "    df2 = df_in.copy(deep=True)  # if not, modify the original dataframe(warning)\n",
    "    df2[list_y] = df_in[list_y].sub(df_in.Ts,axis=0)\n",
    "    df_out = df2\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  subroutine diff_model_data(ydiff, df_in)\n",
    "- Make differences between predicted and target and merge with the initial dataframes \n",
    "- return the concatenated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Make differences between predicted and target and merge with the initial dataframes step 7.4\n",
    "# ydiff is obstained by :\n",
    "# ypred = model.predict (x)  where x is normalized\n",
    "# diff = ypred - y           where y is the  initial data once sorted and eventually in difference with surface temperature\n",
    "\n",
    "def diff_model_data(ydiff, df_in):\n",
    "\n",
    "    nbrow = ydiff.shape[1]   # number of elements in the vertical\n",
    "    \n",
    "    columns_diff = ['mse_row','sigma_row','mae_row','me_row','dT_01', 'dT_02', 'dT_03', 'dT_04', 'dT_05',\n",
    "                'dT_06', 'dT_07', 'dT_08', 'dT_09', 'dT_10', 'dT_11', 'dT_12', 'dT_13', 'dT_14',\n",
    "                'dT_15', 'dT_16', 'dT_17', 'dT_18', 'dT_19', 'dT_20']             # to name the diff columns\n",
    "    \n",
    "    # the bloc below calculate mse_row, mae_row, me_row and stack them with the ydiff 2D values\n",
    "    # each row represent a physical vertical column\n",
    "    \n",
    "    yd2    = ydiff**2\n",
    "    ydabs = np.abs(ydiff)\n",
    "    mse_row   = yd2.sum(axis=1)/nbrow        # mse row\n",
    "    sigma_row = np.sqrt(mse_row)             # sigma_row\n",
    "    mae_row   = ydabs.sum(axis=1)/nbrow      # mae row\n",
    "    me_row    = ydiff.sum(axis=1)/nbrow      # difference with sign\n",
    "    \n",
    "    all_diff = np.column_stack((mse_row,sigma_row,mae_row,me_row,ydiff))\n",
    "    \n",
    "    ### Merge differences with the initial dataframe df_in\n",
    "    ddiff = pd.DataFrame(all_diff, columns=columns_diff,index=df_in.index)\n",
    "    df_out = pd.concat([df_in,ddiff],axis=1,ignore_index=False)\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subroutine get_norm : normalize df according to parameters and norms from dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(df,dic):    # calculate norms from df according to parameters\n",
    "                         # in dic and return x_norm the dataframe with \n",
    "                         # the normalized columns of list_pred\n",
    "    \n",
    "    normalize = dic['normalization']['method']\n",
    "    list_pred = dic['x_columns']['list_pred']   # to reconstruct the mean dataframe\n",
    "\n",
    "\n",
    "    if normalize == 'mean':  # mean std normalisation\n",
    "\n",
    "        # mean_list and std_list are list\n",
    "        mean_list = dic['normalization']['xmean']\n",
    "        std_list  = dic['normalization']['xstd']\n",
    "\n",
    "        # to convert list to dataframe\n",
    "        mean = pd.DataFrame(mean_list,index=list_pred)\n",
    "        std = pd.DataFrame(std_list, index=list_pred)\n",
    "        mean = mean.squeeze()\n",
    "        std = std.squeeze()       \n",
    "\n",
    "        #keep only only the columns 'list_pred' in df\n",
    "        x_norm = (pd.Dataframe(df, columns=list_pred) - mean ) /std\n",
    "\n",
    " \n",
    "\n",
    "    elif normalize == 'meanlog': # replace with the log10 columns from list_meanlog\n",
    "                                 # limited to 1.e-7 to avoid log10 warning\n",
    "                                 # then apply usual mean and std on all columns \n",
    "\n",
    "        list_meanlog = dic['normalization']['columns']\n",
    "        mean_list = dic['normalization']['xmean']\n",
    "        std_list  = dic['normalization']['xstd']\n",
    "\n",
    "        #keep only only the columns 'list_pred' in df\n",
    "        xx = pd.DataFrame(df, columns=list_pred)\n",
    "\n",
    "        # take the log x_train\n",
    "        x_log = xx.copy(deep=True)       # to avoid changing xx\n",
    "        x_tolog   = x_log[list_meanlog]  # keep only the columns that must be logDNN_with_varT\n",
    "        x_tolog   = np.maximum(x_tolog, 1.e-7)  # to avoid 0\n",
    "        x_logged  = np.log10(x_tolog)\n",
    "\n",
    "        \n",
    "        x_log[list_meanlog] = x_logged\n",
    " \n",
    "        # to convert list to dataframe\n",
    "#       pivot constructs a dataframe with columns    \n",
    "#        mean = pd.DataFrame(mean_list).pivot_table(columns=list_pred)\n",
    "#        std  = pd.DataFrame(std_list).pivot_table(columns=list_pred)\n",
    "        mean = pd.DataFrame(mean_list,index=list_pred)\n",
    "        std = pd.DataFrame(std_list, index=list_pred)\n",
    "        mean = mean.squeeze()\n",
    "        std = std.squeeze()    \n",
    " \n",
    "        x_norm = (x_log - mean) / std  \n",
    " \n",
    "    elif normalize == 'minmax': # min-max normalisation\n",
    "        \n",
    "        minx_list = dic['normalization']['minx']\n",
    "        maxx_list = dic['normalization']['maxx']\n",
    "      \n",
    "        minx = pd.DataFrame(minx_list, index=list_pred)\n",
    "        maxx = pd.DataFrame(maxx_list, index=list_pred)\n",
    "        minx = minx.squeeze()\n",
    "        maxx = maxx.squeeze()\n",
    "        \n",
    "        #keep only only the columns 'list_pred' in df\n",
    "        xx = pd.Dataframe(df, columns=list_pred)\n",
    "        \n",
    "        x_norm = (xx - minx) / (maxx - minx)\n",
    "        \n",
    "    return x_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. define title and parameters of the run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title and list of GRISLI files to use \n",
    "\n",
    "\n",
    "- conditions on the nodes (thickness min, type of base)  \n",
    "\n",
    "\n",
    "- list of predictors   \n",
    "    *  possibilities: 'x', 'y', 'S', 'H', 'B', 'Mk', 'Ts', 'Tb', 'Bm', 'ghf', 'slope', 'Us',\n",
    "       'VarTb', 'Bmin', 'Bmax', 'Bminmax', 'Bmedian', 'Bmean', 'Bstd', \n",
    "       'crx','cry','crxy' \n",
    "    * types of regularisation. possibilities mean, meanlog or minmax\n",
    "    \n",
    "    \n",
    "- list of target   \n",
    "    * the vertical temperature profile with parameters\n",
    "    * with some parameters\n",
    "    \n",
    "    \n",
    "- parameters will be stored in a dictionnary docDNN later writen in a json file      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 define all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop on the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirmodel = 'apr-2021-hdf5-json/'\n",
    "dirmodel = ''\n",
    "subtitle = 'april2021_test_'\n",
    "postitle = '_all_meanlog'\n",
    "\n",
    "#ndeb = 7\n",
    "#nend = 17\n",
    "ndeb = 30\n",
    "nend = 40\n",
    "\n",
    "for i in range(ndeb,nend):\n",
    "    # retrieve the title\n",
    "    numstring = \"{:02n}\".format(i)\n",
    "    title = dirmodel + subtitle + numstring + postitle  \n",
    "    print (title)  \n",
    "    \n",
    "    # data files from GRISLI\n",
    "    file08 = './data/Grisli_4_DNN_AN40C008.nc'\n",
    "    file09 = './data/Grisli_4_DNN_AN40C009.nc'\n",
    "    file10 = './data/Grisli_4_DNN_AN40C010.nc'\n",
    "    file11 = './data/Grisli_4_DNN_AN40C011.nc'\n",
    "\n",
    "    ncfile_list = [file08,file09,file10,file11]\n",
    "    \n",
    "    ### apply conditions on nodes \n",
    "    mk = 'grounded'    \n",
    "    Hmin = 1000\n",
    "    base_type = 'all' # 'all' # 'cold', wet' or 'all' \n",
    "    DTb = 0  # takes only point that have neighbours with the same thermal condition\n",
    "\n",
    "    ### characteristics that are used in predictor\n",
    "\n",
    "    # possibilities : 'x', 'y', 'S', 'H', 'B', 'Mk', 'Ts', 'Tb', 'Bm', 'ghf', 'slope', 'Us',\n",
    "    #       'VarTb', 'Bmin', 'Bmax', 'Bminmax', 'Bmedian', 'Bmean', 'Bstd', \n",
    "    #       'crx','cry','crxy' \n",
    "\n",
    "    list_pred = ['H','Ts', 'Bm', 'ghf', 'slope', 'Us']  \n",
    "    list_Tann =['Tann_01kyr', 'Tann_02kyr', 'Tann_03kyr', 'Tann_04kyr', 'Tann_05kyr',\n",
    "                'Tann_06kyr', 'Tann_07kyr', 'Tann_08kyr', 'Tann_09kyr', 'Tann_10kyr', \n",
    "                'Tann_11kyr', 'Tann_12kyr', 'Tann_13kyr', 'Tann_14kyr', 'Tann_15kyr',\n",
    "                'Tann_16kyr', 'Tann_17kyr', 'Tann_18kyr', 'Tann_19kyr', 'Tann_20kyr',\n",
    "                'Tann_21kyr', 'Tann_22kyr', 'Tann_23kyr', 'Tann_24kyr']\n",
    "\n",
    "    list_pred = list_pred + list_Tann\n",
    "    num_x = len(list_pred)   # number of predictors\n",
    "\n",
    "    # type of regularisation\n",
    "    utaub = 'none'\n",
    "    normalize = 'meanlog' #  possibilities mean, meanlog or minmax\n",
    "    list_meanlog = ['Bm','slope','Us'] # characteristics for which meanlog is used\n",
    "\n",
    "\n",
    "    # target : output vector\n",
    "    list_target = ['T_00', 'T_01', 'T_02', 'T_03', 'T_04', 'T_05', 'T_06','T_07', 'T_08', 'T_09', \n",
    "                   'T_10', 'T_11', 'T_12', 'T_13', 'T_14', 'T_15','T_16', 'T_17', 'T_18', 'T_19', 'T_20']\n",
    "\n",
    "    itop = 1  # 1-> does not try to fit the surface temperature\n",
    "    ibottom = 20\n",
    "    nbvect = ibottom - itop + 1   # number of elements in the output vector\n",
    "    list_y = list_target[itop:ibottom+1]\n",
    "    # print(list_y)\n",
    "\n",
    "    # date\n",
    "    today = date.today()\n",
    "    today = today.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    ### 1.2 store parameters in a dictionnary\n",
    "\n",
    "    #- normalisation dictionnary is done in the docDNN later\n",
    "    # fill the parameters in the dictionnary. The dictionnary will be writen in a .json file\n",
    "\n",
    "    # fill the parameters in the dictionnary. The dictionnary will be writen in a .json file\n",
    "    # each topic is a subdictionnary\n",
    "\n",
    "\n",
    "    docDNN = {}                     # create a dictionnary docDNN\n",
    "    docDNN['titles'] = {'main': title,'date': today }           # create and fill a key titles\n",
    "    docDNN['imported_data'] = {'file_list': ncfile_list}\n",
    "    docDNN['select_data'] = {'thickmin': Hmin,'base': base_type,'mk': mk,'varTb': DTb }\n",
    "\n",
    "    docDNN['x_columns'] = {'list_pred': list_pred,'Utaub': utaub}\n",
    "    docDNN['y_columns'] = {'itop': itop,'ibottom':ibottom,'list_y': list_y}\n",
    "\n",
    "    docDNN['normalization'] = {'method': normalize}   # the end of normalization is stored later \n",
    "\n",
    "    ## 2. Import the data to build the model\n",
    "    # Read thet netcdf and convert in a dataframe\n",
    "    # loop on the grisli databases (ncfiles)\n",
    "    # in the subroutine nc2df (file_in)\n",
    "\n",
    "    #At the end of this bloc, <span style='color:blue '> **df_tot** is the dataframe containing all the data </span>\n",
    "    \n",
    "    # initialize DataFrame before loop\n",
    "    df_tot = pd.DataFrame()\n",
    "\n",
    "    # loop on the ncfiles\n",
    "    for file_in in ncfile_list:\n",
    "#        print (file_in)\n",
    "        pd_onefile = nc2pdtd(file_in)\n",
    "        df_tot = df_tot.append(pd_onefile,ignore_index=True)  \n",
    "        \n",
    "    ### columns of the initial database df_tot\n",
    "    #df_tot.columns\n",
    "    \n",
    "    ## 3. Prepare the data\n",
    "    ### 3.1 remove floating part, thickness too small and eventually other caracteristics\n",
    "\n",
    "    # consider removing points where VarTb is too high\n",
    " \n",
    "    data2 = select_some(df_tot,docDNN)\n",
    "    # data2.columns\n",
    "    #display(data2.describe().style.format(\"{0:.2f}\").set_caption(\"data2\"))\n",
    "    \n",
    "    ### 3.2 to work in difference with surface temperature \n",
    "\n",
    "    data2 = diff_surf(data2,docDNN )\n",
    "    # data2.shape\n",
    "\n",
    "    ### 3.3 Split data\n",
    "    # At the end of this bloc  data_train and data_test\n",
    "\n",
    "    data_train = data2.sample(frac=0.7, axis=0)   # axis = 0 for the index\n",
    "    data_test  = data2.drop(data_train.index)\n",
    "    \n",
    "    ### 3.4 split x (predictor) and y (target)\n",
    "\n",
    "    # keep only some of the input and output columns (according to parameters)\n",
    "    # predictors in x\n",
    "    # target in y\n",
    "    # At the end of this bloc  dataframes x_train, x_test, y_train, y_test\n",
    "    # num_x is the number of columns in x_train\n",
    "    x_train = pd.DataFrame(data_train,columns=list_pred)  \n",
    "    x_test = pd.DataFrame(data_test,columns=list_pred)  \n",
    "\n",
    "    # nbvect is the number of columns in y_train\n",
    "    y_train = pd.DataFrame(data_train,columns=list_y)  \n",
    "    y_test = pd.DataFrame(data_test,columns=list_y)  \n",
    "\n",
    "    # eventually add Utaub and/or U2 (see DNN-tests-TGrisli notebook)\n",
    "    #x_train.columns\n",
    "    \n",
    "    ### 3.5 - Data normalization\n",
    "    # display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n",
    "    \n",
    "    # normalization \n",
    "\n",
    "    if normalize == 'mean':  # mean std normalisation\n",
    "        mean = x_train.mean()\n",
    "        std  = x_train.std()\n",
    "        x_trainnorm = (x_train - mean) / std\n",
    "        x_testnorm  = (x_test  - mean) / std  # note x_test is normalized with the\n",
    "                                              # x_train mean and std\n",
    "\n",
    "        mean_list = mean.values.tolist()      # json only accept lists\n",
    "        std_list  = std.values.tolist()\n",
    "\n",
    "        docDNN['normalization']['xmean'] = mean_list\n",
    "        docDNN['normalization']['xstd'] = std_list\n",
    "\n",
    "\n",
    "    elif normalize == 'meanlog': # replace with the log10 columns from list_meanlog\n",
    "                                 # limited to 1.e-7 to avoid log10 warning\n",
    "                                 # then apply usual mean and std on all columns \n",
    "\n",
    "        # take the log x_train\n",
    "        x_trainlog = x_train.copy(deep=True)\n",
    "        train_tolog   = x_trainlog[list_meanlog]  # keep only the columns that must be log\n",
    "        train_tolog   = np.maximum(train_tolog, 1.e-7)  # to avoid 0\n",
    "        train_logged  = np.log10(train_tolog)\n",
    "\n",
    "        x_trainlog[list_meanlog] = train_logged\n",
    "\n",
    "        mean = x_trainlog.mean()\n",
    "        std  = x_trainlog.std()   \n",
    "\n",
    "\n",
    "        x_trainnorm = (x_trainlog - mean) / std\n",
    "\n",
    "\n",
    "        # x_test\n",
    "        x_testlog = x_test.copy(deep=True)\n",
    "        test_tolog   = x_testlog[list_meanlog]   # keep only velocity and slope\n",
    "        test_tolog   = np.maximum(test_tolog, 1.e-7)  # to avoid 0\n",
    "        test_logged  = np.log10(test_tolog)    \n",
    "        x_testlog[list_meanlog] = test_logged\n",
    "\n",
    "\n",
    "        x_testnorm  = (x_testlog  - mean) / std  # note x_test is normalized with \n",
    "                                                 # the x_train mean and std\n",
    "\n",
    "    #   save the normalisation values in a list \n",
    "        mean_list = mean.values.tolist()\n",
    "        std_list  = std.values.tolist()\n",
    "\n",
    "        docDNN['normalization']['columns'] = list_meanlog\n",
    "        docDNN['normalization']['xmean'] = mean_list\n",
    "        docDNN['normalization']['xstd'] = std_list\n",
    "\n",
    "\n",
    "\n",
    "    elif normalize == 'minmax': # min-max normalisation\n",
    "        minx = x_train.min()\n",
    "        maxx = x_train.max()\n",
    "        x_trainnorm = (x_train - minx) / (maxx - minx)\n",
    "        x_testnorm = (x_test - minx)/ (maxx - minx)\n",
    "\n",
    "        minx_list = minx.values.tolist()\n",
    "        maxx_list = maxx.values.tolist()\n",
    "\n",
    "\n",
    "        docDNN['normalization']['minx'] = minx_list\n",
    "        docDNN['normalization']['maxx'] = maxx_list \n",
    "        \n",
    "    #### 3.5.2 Save the characteristics of the run including normalization: mean_list, std_list\n",
    "    \n",
    "    with open(title + '.json', 'w') as outfile:             # parameters of input preprocesing in json \n",
    "        json.dump(docDNN, outfile,indent=4)\n",
    "        \n",
    "    #display(x_trainnorm.describe().style.format(\"{0:.2f}\").set_caption(\"after normalization :\"))\n",
    "\n",
    "    ## 3.6 back to numpy arrays\n",
    "    x_train2 = np.array(x_trainnorm)\n",
    "    x_test2  = np.array(x_testnorm)\n",
    "\n",
    "    y_train2 = np.array(y_train)\n",
    "    y_test2  = np.array(y_test)\n",
    "\n",
    "    x_traintrue = np.array(x_train) \n",
    "    x_testtrue  = np.array(x_test)\n",
    "    \n",
    "    ## 4. Build a model\n",
    "    def get_model_v1(shape):\n",
    "        nodes = 256\n",
    "        #activ = 'sigmoid'\n",
    "        activ = 'relu'   # standard\n",
    "        #activ = 'tanh'\n",
    "        #activ = 'selu'\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n1'))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n2'))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n3'))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n4'))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n5'))\n",
    "        model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n6'))\n",
    "        model.add(keras.layers.Dense(nbvect, name='Output'))  \n",
    "                                        # nbvect = number of elements of target vector\n",
    "\n",
    "        model.compile(optimizer = 'rmsprop',\n",
    "                      loss      = 'mse',                  # mse, mean quadratic error \n",
    "                  metrics   = ['mae', 'mse'] )        # mae =  mean absolute error\n",
    "        return model\n",
    " \n",
    "    ### 5.1  Get the model and show a display of the model\n",
    "    model=get_model_v1( (num_x,) )  # num_x number of predictors = shape of x_train or x_test. \n",
    "\n",
    "    ### show a display of the model \n",
    "\n",
    "    model.summary()\n",
    "    #img=keras.utils.plot_model( model, to_file='./run/model.png', \n",
    "    #                           show_shapes=True, show_layer_names=True, dpi=96)\n",
    "    #display(img)\n",
    "    \n",
    "    \n",
    "\n",
    "    ### 5.2 Define model check points \n",
    "    # mcp : model check point to save the best model\n",
    "\n",
    "\n",
    "\n",
    "    filepath=title+\"-weights-impr-{epoch:02d}-{val_mae:.2f}.hdf5\"\n",
    "    # print(filepath)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                 monitor='val_mae', \n",
    "                                                 verbose=1, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "    ### 5.3 - Train the model\n",
    "\n",
    "    history = model.fit(x_train2,\n",
    "                        y_train2,\n",
    "                        epochs          = 100,\n",
    "                        batch_size      = 20,\n",
    "                        verbose         = 1,\n",
    "                        validation_data = (x_test2, y_test2),\n",
    "                        callbacks       = callbacks_list)\n",
    "    ## 6.  Evaluate\n",
    "\n",
    "    ### 6.1 - Model evaluation\n",
    "    print ('evaluation ', title)\n",
    "    score = model.evaluate(x_test2, y_test2, verbose=0)\n",
    "\n",
    "    print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "    print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "    print('x_test / mse       : {:5.4f}'.format(score[2]))\n",
    "\n",
    "    ### 6.2 - Training history\n",
    "\n",
    "    df=pd.DataFrame(data=history.history)\n",
    "    df.describe()\n",
    "\n",
    "    # print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )\n",
    "    %matplotlib inline\n",
    "    ooo.plot_history(history, plot={'MSE' :['val_mse', 'mse'],\n",
    "                                'MAE' :['val_mae', 'mae']})\n",
    "    \n",
    "    ### 6.3 Retrieve the best model obtained\n",
    "\n",
    "    # it is the last written with filepath : bestmodel\n",
    "    # remove the other models saved from the same training\n",
    "    dirs = os.listdir()   # list the current dir\n",
    "\n",
    "    # sorted(dirs)          # liste dirs sorted \n",
    "\n",
    "    sub = title+'-weight'                                # select the model names, order and take the last one\n",
    "    list_models = sorted([s for s in dirs if sub in s])  # if sub in s ->  if sub is a substring of s\n",
    "#    print(dirs,sub)\n",
    "#    print (list_models)\n",
    "    bestmodel = list_models[-1]                          # take the last one which is the best\n",
    "\n",
    "    remove_files=list_models[0:len(list_models)-1]\n",
    "    for s in remove_files:\n",
    "        os.remove(s)\n",
    "    print('bestmodel=' , bestmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
