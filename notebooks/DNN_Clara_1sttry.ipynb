{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 09 14:36 2021\n",
    "\n",
    "Prepare proof of concept with a very simple DNN to parameterise the sub-shelf melt\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data='/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/NEMO_eORCA025.L121_ANT_STEREO/'\n",
    "inputpath_mask = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/ANTARCTICA_IS_MASKS/nemo_5km_withdask/'\n",
    "inputpath_profiles = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/T_S_PROF/nemo_5km/'\n",
    "inputpath_plumes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/PLUMES/nemo_5km/'\n",
    "inputpath_boxes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/BOXES/nemo_5km/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR EACH POINT:\n",
    "- T and S profiles at the front (decompose z dimension into single things)\n",
    "- Distance to front\n",
    "- Distance to the grounding line\n",
    "- Local slope ice draft\n",
    "- Local slope bedrock\n",
    "- Ice draft depth\n",
    "- Bathymetry\n",
    "- Ice draft concentration\n",
    "- Horizontal coordinates (lon, lat)\n",
    "- Mean bathymetry at entry (to add in future)\n",
    "- Max bathymetry (to add in future)\n",
    "- Target: melt m ice per yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dIF, dGL, longitude, latitude\n",
    "file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T and S profiles\n",
    "file_TS_orig = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_and_offshore_1980-2018.nc')\n",
    "file_TS = file_TS_orig.sel(Nisf=file_isf.Nisf)\n",
    "file_TS_dom = file_TS.sel(profile_domain=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_charac_all_2D = xr.open_dataset(inputpath_boxes + 'nemo_5km_boxes_2D.nc')\n",
    "box_charac_all_1D = xr.open_dataset(inputpath_boxes + 'nemo_5km_boxes_1D.nc')\n",
    "plume_charac = xr.open_dataset(inputpath_plumes+'nemo_5km_plume_characteristics.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local slope\n",
    "local_ice_slope = plume_charac['alpha'].sel(option='appenB').drop('option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_domain_stereo(var_to_cut, map_lim_x, map_lim_y):\n",
    "    var_cutted = var_to_cut.sel(x=var_to_cut.x.where(in_range(var_to_cut.x,map_lim_x),drop=True), y=var_to_cut.y.where(in_range(var_to_cut.y,map_lim_y),drop=True))\n",
    "    return var_cutted\n",
    "\n",
    "def in_range(in_xy,txy):\n",
    "    return ((in_xy >= min(txy)) & (in_xy < max(txy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]\n",
    "file_mask_orig = xr.open_dataset(inputpath_data+'other_mask_vars_Ant_stereo.nc')\n",
    "file_mask_orig_cut = cut_domain_stereo(file_mask_orig, map_lim, map_lim)\n",
    "file_other = xr.open_dataset(inputpath_data+'corrected_draft_bathy_isf.nc')#, chunks={'x': chunk_size, 'y': chunk_size})\n",
    "file_other_cut = cut_domain_stereo(file_other, map_lim, map_lim)\n",
    "file_conc = xr.open_dataset(inputpath_data+'isfdraft_conc_Ant_stereo.nc')\n",
    "file_conc_cut = cut_domain_stereo(file_conc, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathymetry, ice draft, concentration\n",
    "file_bed_orig = file_mask_orig_cut['bathy_metry']\n",
    "file_draft = file_other_cut['corrected_isfdraft'] \n",
    "file_isf_conc = file_conc_cut['isfdraft_conc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bedrock_slope = xr.open_dataset(inputpath_mask+'nemo_5km_bedrock_slope.nc')\n",
    "local_bedrock_slope = file_bedrock_slope['bedrock_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_files = list(sorted(glob.glob(inputpath_data+'cavity_melt_*_Ant_stereo.nc')))\n",
    "ds_melt  = xr.open_mfdataset(melt_files, concat_dim='new_time', combine='nested')#, chunks={'x': chunksize, 'y': chunksize})\n",
    "ds_melt = ds_melt.squeeze('time')\n",
    "ds_melt = ds_melt.rename({'new_time': 'time'})\n",
    "ds_melt = ds_melt.assign_coords(time=np.arange(1980, 2019))\n",
    "ds_melt_cutted = cut_domain_stereo(ds_melt, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_rate = (-1*(ds_melt_cutted*yearinsec/rho_i)/file_isf_conc).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all 2D data in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D = file_isf[['dGL', 'dIF', 'longitude', 'latitude']].merge(local_ice_slope).merge(local_bedrock_slope).merge(file_draft).merge(file_bed_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBSAMPLE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one ice shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisf_of_int = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D_isf = geometry_2D.where(file_isf['ISF_mask'] == kisf_of_int, drop=True)\n",
    "melt_rate_isf = melt_rate.where(file_isf['ISF_mask'] == kisf_of_int, drop=True).load()\n",
    "TS_isf = file_TS_dom.sel(Nisf=kisf_of_int)\n",
    "max_front_depth = file_isf['front_bot_depth_max'].sel(Nisf=kisf_of_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one time step for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 0\n",
    "melt_rate_isf_tt = melt_rate_isf.isel(time=tt)\n",
    "TS_isf_tt = TS_isf.isel(time=tt).where(TS_isf.depth < max_front_depth, drop=True).drop('profile_domain').drop('Nisf').drop('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_list = [ ]\n",
    "S_list = [ ]\n",
    "depth_list = [ ]\n",
    "for ii in range(len(TS_isf_tt.depth)):\n",
    "    T_list.append('T_'+str(ii).zfill(3))\n",
    "    S_list.append('S_'+str(ii).zfill(3))\n",
    "    depth_list.append('d_'+str(ii).zfill(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert T and S to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_isf_df = TS_isf_tt.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 2D time-independent data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = len(geometry_2D_isf.x)*len(geometry_2D_isf.y)\n",
    "\n",
    "geo_df = geometry_2D_isf.drop('x').drop('y').to_dataframe()\n",
    "\n",
    "for nn in range(length_df):\n",
    "    for ii,icol in enumerate(T_list):\n",
    "        geo_df[icol] = TS_isf_df['theta_ocean'].values[ii]\n",
    "    for ii,icol in enumerate(S_list):    \n",
    "        geo_df[icol] = TS_isf_df['salinity_ocean'].values[ii]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert melt to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_df = melt_rate_isf_tt.drop('x').drop('y').drop('longitude').drop('latitude').to_dataframe().drop(['mapping'],axis=1).reset_index().drop(['time'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all and clean NaN-rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(geo_df,melt_df,how='left',on=['x','y'])\n",
    "\n",
    "clean_df = merged_df.dropna()\n",
    "clean_df = clean_df.drop(['x'], axis=1).drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = clean_df.sample(frac=0.7, axis=0) \n",
    "data_test  = clean_df.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_cavity']\n",
    "x_train = data_train.drop(['melt_cavity'], axis=1)\n",
    "\n",
    "y_test = data_test['melt_cavity']\n",
    "x_test = data_test.drop(['melt_cavity'], axis=1)\n",
    "\n",
    "print('Original data shape was : ',clean_df.shape)\n",
    "print('x_train : ',x_train.shape, 'y_train : ',y_train.shape)\n",
    "print('x_test  : ',x_test.shape,  'y_test  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Data normalization\n",
    "**Note :** \n",
    " - All input data must be normalized, train and test.  \n",
    " - To do this we will **subtract the mean** and **divide by the standard deviation**.  \n",
    " - But test data should not be used in any way, even for normalization.  \n",
    " - The mean and the standard deviation will therefore only be calculated with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n",
    "\n",
    "\n",
    "x_train_norm = x_train.copy()\n",
    "x_test_norm = x_test.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude']:\n",
    "    mean = x_train[ccol].mean()\n",
    "    std  = x_train[ccol].std()\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean) / std\n",
    "    x_test_norm[ccol]  = (x_test[ccol]  - mean) / std\n",
    "\n",
    "mean_T = x_train[T_list].mean().mean()\n",
    "std_T = x_train[T_list].mean().std()\n",
    "mean_S = x_train[S_list].mean().mean()\n",
    "std_S = x_train[S_list].mean().std()\n",
    "\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean_T) / std_T\n",
    "    x_test_norm[ccol] = (x_test[ccol] - mean_T) / std_T\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    x_train_norm[ccol] = (x_train[ccol] - mean_S) / std_S\n",
    "    x_test_norm[ccol] = (x_test[ccol] - mean_S) / std_S\n",
    "\n",
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "#display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "\n",
    "x_train_arr, y_train_arr = np.array(x_train_norm), np.array(y_train)\n",
    "x_test_arr,  y_test_arr  = np.array(x_test_norm),  np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x_train_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_v1( (input_size,) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_arr,\n",
    "                    y_train_arr,\n",
    "                    epochs          = 60,\n",
    "                    batch_size      = 10,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_arr, y_test_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_arr, y_test_arr, verbose=1)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_val = 20\n",
    "\n",
    "melt_rate_isf_tt_val = melt_rate_isf.isel(time=tt_val)\n",
    "TS_isf_tt_val = TS_isf.isel(time=tt_val).where(TS_isf.depth < max_front_depth, drop=True).drop('profile_domain').drop('Nisf')\n",
    "\n",
    "TS_isf_df_val = TS_isf_tt_val.to_dataframe()\n",
    "\n",
    "melt_df_val = melt_rate_isf_tt_val.drop('x').drop('y').drop('longitude').drop('latitude').to_dataframe().drop(['mapping'],axis=1).reset_index().drop(['time'],axis=1)\n",
    "\n",
    "length_df = len(geometry_2D_isf.x)*len(geometry_2D_isf.y)\n",
    "\n",
    "geo_df_val = geo_df.copy()\n",
    "for nn in range(length_df):\n",
    "    for ii,icol in enumerate(T_list):\n",
    "        geo_df_val[icol] = TS_isf_df_val['theta_ocean'].values[ii]\n",
    "    for ii,icol in enumerate(S_list):    \n",
    "        geo_df_val[icol] = TS_isf_df_val['salinity_ocean'].values[ii]\n",
    "        \n",
    "merged_df_val = pd.merge(geo_df_val,melt_df_val,how='left',on=['x','y'])\n",
    "\n",
    "clean_df_val = merged_df_val.dropna()\n",
    "clean_df_val = clean_df_val.drop(['x'], axis=1).drop(['y'], axis=1)\n",
    "\n",
    "y_val = clean_df_val['melt_cavity']\n",
    "x_val = clean_df_val.drop(['melt_cavity'], axis=1)\n",
    "\n",
    "x_val_norm = x_val.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude']:\n",
    "    mean = x_train[ccol].mean()\n",
    "    std  = x_train[ccol].std()\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean) / std\n",
    "\n",
    "mean_T = x_train[T_list].mean().mean()\n",
    "std_T = x_train[T_list].mean().std()\n",
    "mean_S = x_train[S_list].mean().mean()\n",
    "std_S = x_train[S_list].mean().std()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean_T) / std_T\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean_S) / std_S\n",
    "\n",
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "#display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "\n",
    "x_val_arr, y_val_arr = np.array(x_val_norm), np.array(y_val)\n",
    "\n",
    "#my_data=np.array(x_val_arr)#.reshape(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = x_val_arr[0,:].reshape(1,input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )\n",
    "print(\"Prediction : {:.2f} m ice per y\".format(predictions[0][0]))\n",
    "print(\"Reality    : {:.2f} m ice per y\".format(y_val_arr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== TO KEEP FOR THE FUTURE ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column - for normalization with min and max\n",
    "\n",
    "normalized_clean_df = clean_df.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude','melt_cavity','time']:\n",
    "    max_ccol = clean_df[ccol].max()\n",
    "    min_ccol = clean_df[ccol].min()\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_ccol)/(max_ccol - min_ccol)\n",
    "\n",
    "max_T = clean_df[T_list].max().max()\n",
    "min_T = clean_df[T_list].min().min()\n",
    "max_S = clean_df[S_list].max().max()\n",
    "min_S = clean_df[S_list].min().min()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_T)/(max_T - min_T)\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_S)/(max_S - min_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
