{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 09 14:36 2021\n",
    "\n",
    "Prepare proof of concept with a very simple DNN to parameterise the sub-shelf melt\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "from basal_melt_neural_networks.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data='/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/NEMO_eORCA025.L121_ANT_STEREO/'\n",
    "inputpath_mask = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/ANTARCTICA_IS_MASKS/nemo_5km_withdask/'\n",
    "inputpath_profiles = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/T_S_PROF/nemo_5km/'\n",
    "inputpath_plumes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/PLUMES/nemo_5km/'\n",
    "inputpath_boxes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/BOXES/nemo_5km/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR EACH POINT:\n",
    "- T and S profiles at the front (decompose z dimension into single things)\n",
    "- Distance to front\n",
    "- Distance to the grounding line\n",
    "- Local slope ice draft\n",
    "- Local slope bedrock\n",
    "- Ice draft depth\n",
    "- Bathymetry\n",
    "- Ice draft concentration\n",
    "- Horizontal coordinates (lon, lat)\n",
    "- Mean bathymetry at entry (to add in future)\n",
    "- Max bathymetry (to add in future)\n",
    "- Target: melt m ice per yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dIF, dGL, longitude, latitude\n",
    "file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T and S profiles\n",
    "file_TS_orig = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_and_offshore_1980-2018.nc')\n",
    "file_TS = file_TS_orig.sel(Nisf=file_isf.Nisf)\n",
    "file_TS_dom = file_TS.sel(profile_domain=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_charac_all_2D = xr.open_dataset(inputpath_boxes + 'nemo_5km_boxes_2D.nc')\n",
    "box_charac_all_1D = xr.open_dataset(inputpath_boxes + 'nemo_5km_boxes_1D.nc')\n",
    "plume_charac = xr.open_dataset(inputpath_plumes+'nemo_5km_plume_characteristics.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local slope\n",
    "local_ice_slope = plume_charac['alpha'].sel(option='appenB').drop('option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_domain_stereo(var_to_cut, map_lim_x, map_lim_y):\n",
    "    var_cutted = var_to_cut.sel(x=var_to_cut.x.where(in_range(var_to_cut.x,map_lim_x),drop=True), y=var_to_cut.y.where(in_range(var_to_cut.y,map_lim_y),drop=True))\n",
    "    return var_cutted\n",
    "\n",
    "def in_range(in_xy,txy):\n",
    "    return ((in_xy >= min(txy)) & (in_xy < max(txy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]\n",
    "file_mask_orig = xr.open_dataset(inputpath_data+'other_mask_vars_Ant_stereo.nc')\n",
    "file_mask_orig_cut = cut_domain_stereo(file_mask_orig, map_lim, map_lim)\n",
    "file_other = xr.open_dataset(inputpath_data+'corrected_draft_bathy_isf.nc')#, chunks={'x': chunk_size, 'y': chunk_size})\n",
    "file_other_cut = cut_domain_stereo(file_other, map_lim, map_lim)\n",
    "file_conc = xr.open_dataset(inputpath_data+'isfdraft_conc_Ant_stereo.nc')\n",
    "file_conc_cut = cut_domain_stereo(file_conc, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathymetry, ice draft, concentration\n",
    "file_bed_orig = file_mask_orig_cut['bathy_metry']\n",
    "file_draft = file_other_cut['corrected_isfdraft'] \n",
    "file_isf_conc = file_conc_cut['isfdraft_conc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bedrock_slope = xr.open_dataset(inputpath_mask+'nemo_5km_bedrock_slope.nc')\n",
    "local_bedrock_slope = file_bedrock_slope['bedrock_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_files = list(sorted(glob.glob(inputpath_data+'cavity_melt_*_Ant_stereo.nc')))\n",
    "ds_melt  = xr.open_mfdataset(melt_files, concat_dim='new_time', combine='nested')#, chunks={'x': chunksize, 'y': chunksize})\n",
    "ds_melt = ds_melt.squeeze('time')\n",
    "ds_melt = ds_melt.rename({'new_time': 'time'})\n",
    "ds_melt = ds_melt.assign_coords(time=np.arange(1980, 2019))\n",
    "ds_melt_cutted = cut_domain_stereo(ds_melt, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_rate = (ds_melt_cutted['melt_cavity'] * yearinsec).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all 2D data in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D = file_isf[['dGL', 'dIF', 'longitude', 'latitude']].merge(local_ice_slope).merge(local_bedrock_slope).merge(file_draft).merge(file_bed_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBSAMPLE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one ice shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisf_of_int = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D_isf = geometry_2D.where(file_isf['ISF_mask'] == kisf_of_int, drop=True)\n",
    "melt_rate_isf = melt_rate.where(file_isf['ISF_mask'] == kisf_of_int, drop=True).load()\n",
    "TS_isf = file_TS_dom.sel(Nisf=kisf_of_int)\n",
    "max_front_depth = file_isf['front_bot_depth_max'].sel(Nisf=kisf_of_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one time step for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 0\n",
    "melt_rate_isf_tt = melt_rate_isf.isel(time=tt)\n",
    "TS_isf_tt = TS_isf.isel(time=tt).where(TS_isf_tt.depth < max_front_depth, drop=True).drop('profile_domain').drop('Nisf').drop('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert T and S to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_isf_df = TS_isf_tt.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_list = [ ]\n",
    "S_list = [ ]\n",
    "depth_list = [ ]\n",
    "for ii in range(len(TS_isf_tt.depth)):\n",
    "    T_list.append('T_'+str(ii).zfill(3))\n",
    "    S_list.append('S_'+str(ii).zfill(3))\n",
    "    depth_list.append('d_'+str(ii).zfill(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 2D time-independent data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = len(geometry_2D_isf.x)*len(geometry_2D_isf.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = geometry_2D_isf.drop('x').drop('y').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nn in range(length_df):\n",
    "    for ii,icol in enumerate(T_list):\n",
    "        geo_df[icol] = TS_isf_df['theta_ocean'].values[ii]\n",
    "    for ii,icol in enumerate(S_list):    \n",
    "        geo_df[icol] = TS_isf_df['salinity_ocean'].values[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert melt to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_df = melt_rate_isf_tt.drop('x').drop('y').drop('longitude').drop('latitude').to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all and clean NaN-rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(geo_df,melt_df,how='left',on=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = merged_df.dropna().reset_index()\n",
    "clean_df = clean_df.drop(['x'], axis=1).drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column\n",
    "\n",
    "normalized_clean_df = clean_df.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude','melt_cavity']:\n",
    "    max_ccol = clean_df[ccol].max()\n",
    "    min_ccol = clean_df[ccol].min()\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_ccol)/(max_ccol - min_ccol)\n",
    "\n",
    "max_T = clean_df[T_list].max().max()\n",
    "min_T = clean_df[T_list].min().min()\n",
    "max_S = clean_df[S_list].max().max()\n",
    "min_S = clean_df[S_list].min().min()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_T)/(max_T - min_T)\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_S)/(max_S - min_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = normalized_clean_df.sample(frac=0.7, axis=0) \n",
    "data_test  = normalized_clean_df.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_cavity'].copy()\n",
    "x_train = data_train.drop(['melt_cavity'], axis=1)\n",
    "\n",
    "y_test = data_test['melt_cavity'].copy()\n",
    "x_test = data_test.drop(['melt_cavity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
