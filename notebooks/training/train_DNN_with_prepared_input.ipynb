{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f8f74-a672-4a47-9867-3c3f1309c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr 05 15:43 2022\n",
    "\n",
    "Script to train DNN on prepared input\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78591024-6443-4390-b166-fdb3091f29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0a826-5e46-4c4c-8f01-8c78f66bb74a",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d67da9-d01e-4850-89c3-739159ea00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = ['OPM006', 'OPM016', 'OPM018', 'OPM031'] #\n",
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd269f-2a36-4405-bbc9-775fd86d7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_timetag = False\n",
    "if new_timetag:\n",
    "    datetag_dt = datetime.datetime.today()\n",
    "    timetag_dt = datetime.datetime.now()\n",
    "    timetag = str(datetag_dt.year)+str(datetag_dt.month).zfill(2)+str(datetag_dt.day).zfill(2)+'-'+str(timetag_dt.hour).zfill(2)+str(timetag_dt.minute).zfill(2)\n",
    "else:\n",
    "    timetag = '20220414-1706'\n",
    "\n",
    "new_path_model = outputpath_nn_models+timetag+'/'\n",
    "if not os.path.isdir(new_path_model):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in model folder so I created a new one, I hope that's ok!\")\n",
    "    os.mkdir(new_path_model)\n",
    "else:\n",
    "    print(\"This folder (\"+timetag+\") in model folder exists already!\")\n",
    "\n",
    "new_path_doc = outputpath_doc+timetag+'/'\n",
    "if not os.path.isdir(new_path_doc):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in doc folder so I created a new one, I hope that's ok!\")\n",
    "    os.mkdir(new_path_doc)\n",
    "else:\n",
    "    print(\"This folder (\"+timetag+\") in doc folder exists already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a3bdb-8197-4d5b-a98a-340bf3ea130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nemo_run in run_list:\n",
    "    \n",
    "    # copy to folder for archive\n",
    "    src = inputpath_data + 'dataframe_input_'+nemo_run+'.csv'\n",
    "    dst = new_path_model + 'dataframe_input_'+nemo_run+'.csv'\n",
    "    os.popen(f\"cp {src} {dst}\")\n",
    "    \n",
    "    print('imported input data to '+nemo_run+' model folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfddc39-d689-4cef-8adc-fe9264784135",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_df = None\n",
    "\n",
    "for nemo_run in run_list:\n",
    "    print(nemo_run)\n",
    "    \n",
    "    # read in the file\n",
    "    clean_df_nrun = pd.read_csv(new_path_model + 'dataframe_input_'+nemo_run+'.csv',index_col=[0,1,2])\n",
    "    clean_df_nrun.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # append the different runs\n",
    "    if all_input_df is None:\n",
    "        all_input_df = clean_df_nrun\n",
    "    else:\n",
    "        all_input_df = all_input_df.append(clean_df_nrun, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f8c3e-3340-4055-9a6e-9ddfd08486b2",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf18d74-d99e-4391-bbe9-0d9fe1d3b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = all_input_df.sample(frac=0.7, axis=0) \n",
    "data_test  = all_input_df.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ea519-b43d-4546-b1cd-9af1c35ffa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_m_ice_per_y']\n",
    "x_train = data_train.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "y_test = data_test['melt_m_ice_per_y']\n",
    "x_test = data_test.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "print('Original data shape was : ',all_input_df.shape)\n",
    "print('x_train : ',x_train.shape, 'y_train : ',y_train.shape)\n",
    "print('x_test  : ',x_test.shape,  'y_test  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ddb85-1188-4090-baae-1747a4ea7fbd",
   "metadata": {},
   "source": [
    "DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398aa257-3916-4797-af33-ed2768c96894",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_method = 'interquart' #'std', 'interquart', 'minmax'\n",
    "\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "\n",
    "if norm_method == 'std':\n",
    "    x_range  = x_train.std()\n",
    "    y_range  = y_train.std()\n",
    "elif norm_method == 'interquart':\n",
    "    x_range  = x_train.quantile(0.9) - x_train.quantile(0.1)\n",
    "    y_range  = y_train.quantile(0.9) - y_train.quantile(0.1)\n",
    "elif norm_method == 'minmax':\n",
    "    x_range  = x_train.max() - x_train.min() \n",
    "    y_range  = y_train.max() - y_train.min() \n",
    "    \n",
    "x_train_norm = (x_train - x_mean)/x_range\n",
    "x_test_norm = (x_test - x_mean)/x_range\n",
    "\n",
    "y_train_norm = (y_train - y_mean)/y_range\n",
    "y_test_norm = (y_test - y_mean)/y_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257277c-e4a9-4dcb-8a5f-6700d39e33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "summary_df['x_mean'] = x_mean\n",
    "summary_df['x_range'] = x_range\n",
    "summary_df = summary_df.T \n",
    "summary_df['melt_m_ice_per_y'] = [y_mean, y_range]\n",
    "summary_df.to_csv(new_path_model + 'dataframe_norm_training_data_'+timetag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f73cbb-9fe2-4138-9779-1d301fece825",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_arr, y_train_arr = np.array(x_train_norm), np.array(y_train_norm)\n",
    "x_test_arr,  y_test_arr  = np.array(x_test_norm),  np.array(y_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333d536-7f21-46e0-9754-4dc8d54f3bf6",
   "metadata": {},
   "source": [
    "BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f9bb2-0efb-473a-95f5-60966575b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_v2(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed41cc-af8d-45bd-9660-f3a1cfbbdb5c",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407a6f7-724f-480a-b114-6d91f17d530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x_train_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7c944-375d-473f-821f-19d33ef51914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_v1( (input_size,) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc4905-b361-4450-b8f3-7a5d6337086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nb = 100\n",
    "batch_siz = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da646a40-1eeb-4518-8f3b-2869993e088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_path_doc+'info_'+timetag+'.log','w') as file:\n",
    "    file.write('Timetag: '+timetag+' \\n')\n",
    "    file.write('----- DATA ----- \\n')\n",
    "    file.write('Training data from: '+str(run_list)+'\\n')\n",
    "    file.write('Norm method: '+norm_method+'\\n')\n",
    "    file.write('Original data shape was : '+str(all_input_df.shape)+'\\n')\n",
    "    file.write('x_train : '+str(x_train.shape)+', y_train : '+str(y_train.shape)+'\\n')\n",
    "    file.write('x_test  : '+str(x_test.shape)+', y_test  : '+str(y_test.shape)+'\\n') \n",
    "    file.write('Input variables: '+','.join(map(str,x_train_norm.columns))+'\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('----- MODEL ----- \\n')\n",
    "    with redirect_stdout(file):\n",
    "        model.summary()\n",
    "    file.write('\\n')\n",
    "    file.write('----- TRAINING ----- \\n')\n",
    "    file.write('Epochs: '+str(epoch_nb)+'\\n')\n",
    "    file.write('Batch size: '+str(batch_siz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cb558-58f3-4b1d-ba2a-8d84eef8e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "\n",
    "time_start = time.time()\n",
    "history = model.fit(x_train_arr,\n",
    "                    y_train_arr,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_arr, y_test_arr),\n",
    "                   callbacks=[reduce_lr])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "with open(outputpath_doc+timetag+'.log','a') as file:\n",
    "    file.write('\\n Training time (in s): '+str(timelength))\n",
    "model.save(new_path_model + 'model_nn_'+timetag+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea900440-fb6c-4a1a-b010-7c8daf559e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = new_path_model+'history_'+timetag+'.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748a4f1-7422-4136-abfe-24ae649a4fff",
   "metadata": {},
   "source": [
    "QUICK EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72954930-0364-4a3c-bcd4-dd5f1e820ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_arr, y_test_arr, verbose=1)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe132e-9f4e-4b14-865a-4ff8bcd35ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60582872-142d-4c61-86b0-820a74aa0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e5c85-518b-40ef-88ae-cfebf516d506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
