{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74bb43-830f-4bce-86c8-ef0b5e8b8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Sep 08 11:15 2022\n",
    "\n",
    "Try out the running script for cross-validation\n",
    "This script is to automate a bit the cross-validation and avoiding having to do it in a notebook\n",
    "\n",
    "Author: Clara Burgard\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from dask import delayed\n",
    "\n",
    "import distributed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt\n",
    "import basal_melt_neural_networks.model_functions as modf\n",
    "\n",
    "######### READ IN OPTIONS\n",
    "\n",
    "mod_size = 'small' #'mini', 'small', 'medium', 'large', 'extra_large'\n",
    "tblock_out = 0\n",
    "isf_out = 10\n",
    "TS_opt = 'extrap' # extrap, whole, thermocline\n",
    "norm_method = 'interquart' # std, interquart, minmax\n",
    "\n",
    "######### READ IN DATA\n",
    "\n",
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'\n",
    "\n",
    "if (tblock_out > 0) and (isf_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_TBLOCK/'\n",
    "    \n",
    "elif (isf_out > 0) and (tblock_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_ISF/'\n",
    "    \n",
    "else:\n",
    "    print(\"I do not know what to do with both tblock and isf left out! \")\n",
    "\n",
    "#new_path_doc = outputpath_doc+timetag+'/'\n",
    "#if not os.path.isdir(new_path_doc):\n",
    "#    print(\"I did not find this folder (\"+timetag+\") in doc folder! :( \")\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    inputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS_CV/'\n",
    "elif TS_opt == 'whole':\n",
    "    inputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    inputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS_CV/'\n",
    "    \n",
    "data_train_norm = xr.open_dataset(inputpath_CVinput + 'train_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "data_val_norm = xr.open_dataset(inputpath_CVinput + 'val_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')   \n",
    "\n",
    "## prepare input and target\n",
    "            \n",
    "y_train_norm = data_train_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()\n",
    "\n",
    "y_val_norm = data_val_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "x_val_norm = data_val_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276f000-9ef3-462d-9a04-16003223290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c6f5b-c796-4701-97a4-776d85505a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TRAIN THE MODEL\n",
    "\n",
    "input_size = x_train_norm.values.shape[0]\n",
    "activ_fct = 'relu' #LeakyReLU\n",
    "epoch_nb = 100\n",
    "batch_siz = 512\n",
    "\n",
    "model = modf.get_model(mod_size, input_size, activ_fct)\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0000001, min_delta=0.0005) #, min_delta=0.1\n",
    "            \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    #min_delta=0.000001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68d608-5e32-476a-9055-213d707d8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2364ca7-f5dc-47a2-9bcf-91fce6696eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 2,\n",
    "                    validation_data = (x_val_norm.T.values, y_val_norm.values),\n",
    "                   callbacks=[reduce_lr, early_stop])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "#with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "#    file.write('\\n Reduce_lr: True')\n",
    "#    file.write('\\n Early_stop: True')\n",
    "#    file.write('\\n Training time (in s): '+str(timelength))\n",
    "#model.save(new_path_model + 'model_nn_'+timetag+'.h5')\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/TEMPORARY/history_test.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f39928-2585-43ce-bba1-4b9b3630a774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
