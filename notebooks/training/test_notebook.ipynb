{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0720e-2884-45d2-9b61-8cb5241bbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Sep 08 11:15 2022\n",
    "\n",
    "This script is to automate a bit the cross-validation and avoiding having to do it in a notebook\n",
    "\n",
    "Author: Clara Burgard\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import basal_melt_neural_networks.model_functions as modf\n",
    "import basal_melt_neural_networks.prep_input_data as indat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d31d0-e9f7-44a5-85b2-da6d3a7f2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### READ IN OPTIONS\n",
    "\n",
    "mod_size = 'mini' #, 'small', 'medium', 'large', 'extra_large'\n",
    "tblock_out = 1\n",
    "isf_out = 0\n",
    "TS_opt = 'whole' # extrap, whole, thermocline\n",
    "norm_method = 'std' # std, interquart, minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e322d39-661a-42ba-9196-54875fbf2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### READ IN DATA\n",
    "\n",
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'\n",
    "\n",
    "tblock_dim = range(1,14)\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "\n",
    "if (tblock_out > 0) and (isf_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_TBLOCK/'\n",
    "    \n",
    "elif (isf_out > 0) and (tblock_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_ISF/'\n",
    "    \n",
    "else:\n",
    "    print(\"I do not know what to do with both tblock and isf left out! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c36d4-af92-45f9-bee5-ddf89c6ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_path_doc = outputpath_doc+timetag+'/'\n",
    "#if not os.path.isdir(new_path_doc):\n",
    "#    print(\"I did not find this folder (\"+timetag+\") in doc folder! :( \")\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    inputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS_CV/'\n",
    "elif TS_opt == 'whole':\n",
    "    inputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    inputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS_CV/'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    data_train_norm = xr.open_dataset(inputpath_CVinput + 'train_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    data_val_norm = xr.open_dataset(inputpath_CVinput + 'val_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc') \n",
    "    \n",
    "    ## prepare input and target\n",
    "            \n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()\n",
    "\n",
    "    y_val_norm = data_train_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "    x_val_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()\n",
    "\n",
    "elif TS_opt == 'whole':\n",
    "\n",
    "    #print(tblock_out)\n",
    "    data_train_norm, data_val_norm = indat.prepare_normed_input_data_CV_metricsgiven(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data, norm_method)\n",
    "    \n",
    "    # dies here already\n",
    "    ## prepare input and target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04ef67-1e01-473f-9d1c-96edefdd1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TS_opt == 'whole':\n",
    "    inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "    inputpath_metrics = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee406d8-4ef6-49f1-b47d-4b216d4849e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ds_all = xr.open_dataset(inputpath_metrics + 'metrics_norm_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90239014-626f-403f-8e90-f59efc3e02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean = summary_ds_all.sel(metric='mean_vars', norm_method=norm_method)\n",
    "var_range = summary_ds_all.sel(metric='range_vars', norm_method=norm_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895422ec-ca27-41a9-a2c0-ff2155def16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('test1')\n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "    print('test2')\n",
    "    y_val_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_val_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "    \n",
    "else:\n",
    "    print('Sorry, I dont know this option for TS input yet, you need to implement it...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95f5b6-9170-498f-a469-812cf5e52761",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TRAIN THE MODEL\n",
    "\n",
    "input_size = x_train_norm.values.shape[0]\n",
    "activ_fct = 'relu' #LeakyReLU\n",
    "epoch_nb = 100\n",
    "batch_siz = 512\n",
    "\n",
    "model = modf.get_model(mod_size, input_size, activ_fct)\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0000001, min_delta=0.0005) #, min_delta=0.1\n",
    "            \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    #min_delta=0.000001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 2,\n",
    "                    validation_data = (x_val_norm.T.values, y_val_norm.values),\n",
    "                   callbacks=[reduce_lr, early_stop])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "#with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "#    file.write('\\n Reduce_lr: True')\n",
    "#    file.write('\\n Early_stop: True')\n",
    "#    file.write('\\n Training time (in s): '+str(timelength))\n",
    "model.save(path_model + 'model_nn_'+mod_size+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = path_model + 'history_'+mod_size+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'_TS'+TS_opt+'_norm'+norm_method+'.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276f000-9ef3-462d-9a04-16003223290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c6f5b-c796-4701-97a4-776d85505a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TRAIN THE MODEL\n",
    "\n",
    "input_size = x_train_norm.values.shape[0]\n",
    "activ_fct = 'relu' #LeakyReLU\n",
    "epoch_nb = 100\n",
    "batch_siz = 512\n",
    "\n",
    "model = modf.get_model(mod_size, input_size, activ_fct)\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0000001, min_delta=0.0005) #, min_delta=0.1\n",
    "            \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    #min_delta=0.000001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68d608-5e32-476a-9055-213d707d8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2364ca7-f5dc-47a2-9bcf-91fce6696eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 2,\n",
    "                    validation_data = (x_val_norm.T.values, y_val_norm.values),\n",
    "                   callbacks=[reduce_lr, early_stop])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "#with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "#    file.write('\\n Reduce_lr: True')\n",
    "#    file.write('\\n Early_stop: True')\n",
    "#    file.write('\\n Training time (in s): '+str(timelength))\n",
    "#model.save(new_path_model + 'model_nn_'+timetag+'.h5')\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/TEMPORARY/history_test.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f39928-2585-43ce-bba1-4b9b3630a774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
