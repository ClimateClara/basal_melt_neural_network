{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0720e-2884-45d2-9b61-8cb5241bbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Sep 08 11:15 2022\n",
    "\n",
    "This script is to automate a bit the cross-validation and avoiding having to do it in a notebook\n",
    "\n",
    "Author: Clara Burgard\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import basal_melt_neural_networks.model_functions as modf\n",
    "import basal_melt_neural_networks.prep_input_data as indat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d31d0-e9f7-44a5-85b2-da6d3a7f2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### READ IN OPTIONS\n",
    "\n",
    "mod_size = 'mini' #, 'small', 'medium', 'large', 'extra_large'\n",
    "tblock_out = 1\n",
    "isf_out = 0\n",
    "TS_opt = 'whole' # extrap, whole, thermocline\n",
    "norm_method = 'std' # std, interquart, minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f5e6-0291-4c14-a8e8-e33159795e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### READ IN DATA\n",
    "\n",
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'\n",
    "\n",
    "tblock_dim = range(1,14)\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "\n",
    "if (tblock_out > 0) and (isf_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_TBLOCK/'\n",
    "    \n",
    "elif (isf_out > 0) and (tblock_out == 0):\n",
    "    path_model = outputpath_nn_models+'CV_ISF/'\n",
    "    \n",
    "else:\n",
    "    print(\"I do not know what to do with both tblock and isf left out! \")\n",
    "\n",
    "#new_path_doc = outputpath_doc+timetag+'/'\n",
    "#if not os.path.isdir(new_path_doc):\n",
    "#    print(\"I did not find this folder (\"+timetag+\") in doc folder! :( \")\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    inputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS_CV/'\n",
    "elif TS_opt == 'whole':\n",
    "    inputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    inputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS_CV/'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    data_train_norm = xr.open_dataset(inputpath_CVinput + 'train_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    data_val_norm = xr.open_dataset(inputpath_CVinput + 'val_data_CV_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc') \n",
    "    \n",
    "    ## prepare input and target\n",
    "            \n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()\n",
    "\n",
    "    y_val_norm = data_train_norm['melt_m_ice_per_y'].sel(norm_method=norm_method).load()\n",
    "    x_val_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).sel(norm_method=norm_method).to_array().load()\n",
    "\n",
    "elif TS_opt == 'whole':\n",
    "\n",
    "    #print(tblock_out)\n",
    "    inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "    ds_all = xr.open_dataset(inputpath_prof + 'dataframe_allisf_tblocks1to13.nc')\n",
    "    ds_idx = xr.open_dataset(inputpath_prof + 'indexing_allisf_tblocks1to13.nc')\n",
    "    data_train_norm, data_val_norm = indat.prepare_normed_input_data_CV_metricsgiven(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data, norm_method, ds_all=ds_all, ds_idx=ds_idx)\n",
    "\n",
    "    ## prepare input and target\n",
    "    \n",
    "    print('test1')\n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "    print('test2')\n",
    "    y_val_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_val_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "    \n",
    "else:\n",
    "    print('Sorry, I dont know this option for TS input yet, you need to implement it...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592be949-395a-4f39-9721-23053aa21c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### TRAIN THE MODEL\n",
    "\n",
    "input_size = x_train_norm.values.shape[0]\n",
    "activ_fct = 'relu' #LeakyReLU\n",
    "epoch_nb = 100\n",
    "batch_siz = 512\n",
    "\n",
    "model = modf.get_model(mod_size, input_size, activ_fct)\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0000001, min_delta=0.0005) #, min_delta=0.1\n",
    "            \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    #min_delta=0.000001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 2,\n",
    "                    validation_data = (x_val_norm.T.values, y_val_norm.values),\n",
    "                   callbacks=[reduce_lr, early_stop])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "#with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "#    file.write('\\n Reduce_lr: True')\n",
    "#    file.write('\\n Early_stop: True')\n",
    "#    file.write('\\n Training time (in s): '+str(timelength))\n",
    "model.save(path_model + 'model_nn_'+mod_size+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = path_model + 'history_'+mod_size+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'_TS'+TS_opt+'_norm'+norm_method+'.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
