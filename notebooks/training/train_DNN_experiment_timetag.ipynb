{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cc4f9-f59e-4f93-991e-3265b669aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr 26 16:18 2022\n",
    "\n",
    "Script to train DNN on prepared input for timetag experiment\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41146b-7a1d-4b21-8158-a8cea6b1b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from dask import delayed\n",
    "\n",
    "import distributed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7826e-bcf4-4262-8a6c-fefb9994d5f4",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c389b-a9f5-425f-9fda-bfffb5a9cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64368c05-a425-46f0-92a0-ad154375ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "timetag = '20220511-1121'\n",
    "mod_size = 'large' #'mini', 'small', 'medium', 'large'\n",
    "\n",
    "new_path_model = outputpath_nn_models+timetag+'/'\n",
    "if not os.path.isdir(new_path_model):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in model folder! :( \")\n",
    "\n",
    "new_path_doc = outputpath_doc+timetag+'/'\n",
    "if not os.path.isdir(new_path_doc):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in doc folder! :( \")\n",
    "    \n",
    "new_path_input = inputpath_data+timetag+'/'\n",
    "if not os.path.isdir(new_path_input):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in input folder! :( \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f60987-4a76-4f59-984e-c4b868547aa8",
   "metadata": {},
   "source": [
    "BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6e3c7-9f6f-4f55-a003-ba19521567ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if timetag in ['20220427-0957','20220427-1052','20220427-1058','20220427-1059','20220511-1121']:\n",
    "    timetag_data = '20220427-0957'\n",
    "elif timetag in ['20220427-1002','20220427-1021','20220427-1042','20220427-1051']:\n",
    "    timetag_data = '20220427-1002'\n",
    "path_data = outputpath_nn_models+timetag_data+'/'\n",
    "data_train_norm = xr.open_dataset(path_data + 'dataset_norm_training_data_'+timetag_data+'.nc')\n",
    "data_test_norm = xr.open_dataset(path_data + 'dataset_norm_test_data_'+timetag_data+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ebcc8-0333-4bdc-b3ac-4b307c3621a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if timetag == '20220511-1121': # train without T and S\n",
    "\n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['theta_in','salinity_in','melt_m_ice_per_y']).to_array().load()\n",
    "\n",
    "    y_test_norm = data_test_norm['melt_m_ice_per_y'].load()\n",
    "    x_test_norm = data_test_norm.drop_vars(['theta_in','salinity_in','melt_m_ice_per_y']).to_array().load()\n",
    "    \n",
    "else:\n",
    "\n",
    "    y_train_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "    x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "\n",
    "    y_test_norm = data_test_norm['melt_m_ice_per_y'].load()\n",
    "    x_test_norm = data_test_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf5137-15e1-4efd-ba6e-02f90e13e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mini(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_small(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_medium(shape):\n",
    "    \n",
    "    activ = 'relu'\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(96, activation = activ, name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(96, activation= activ, name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(96, activation= activ, name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(96, activation= activ, name='Dense_n4'))\n",
    "    model.add(keras.layers.Dense(96, activation= activ, name='Dense_n5'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))  \n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_large(shape):\n",
    "    \n",
    "    activ = 'relu'\n",
    "    nodes = 256\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n4'))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n5'))\n",
    "    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n6'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))  \n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e04857-8fe8-4487-b3c7-38aafd58d0bb",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218b3d5-ca83-4b4e-b716-1cd1add0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train_norm.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c861a-9943-4fab-b975-1f3c1ec65130",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mod_size == 'mini':\n",
    "    model=get_model_mini( (input_size,) )\n",
    "elif mod_size == 'small':\n",
    "    model=get_model_small( (input_size,) )\n",
    "elif mod_size == 'medium':\n",
    "    model=get_model_medium( (input_size,) )\n",
    "elif mod_size == 'large':\n",
    "    model=get_model_large( (input_size,) )\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321c96e-6741-4c0f-ad0b-00f6f2ab0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nb = 100\n",
    "batch_siz = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397d412-41a3-4664-b6c4-b14a3866aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                              patience=5, min_lr=0.001)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0000001, min_delta=0.0005) #, min_delta=0.1\n",
    "            \n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    #min_delta=0.000001,\n",
    "    patience=6,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_norm.T.values, y_test_norm.values),\n",
    "                   callbacks=[reduce_lr, early_stop])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "    file.write('\\n Reduce_lr: True')\n",
    "    file.write('\\n Early_stop: True')\n",
    "    file.write('\\n Training time (in s): '+str(timelength))\n",
    "model.save(new_path_model + 'model_nn_'+timetag+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124eeb4a-6792-4ab0-9bbd-97f837cc8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = new_path_model+'history_'+timetag+'.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e0d9f-8273-429d-8765-d4a50dd3da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "timetag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602ea87-d441-4c14-bb9c-424c266bfb54",
   "metadata": {},
   "source": [
    "QUICK CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2052f9-3a50-40b7-853f-f24cbf0ce760",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424a27d-70bd-4184-a768-7624c1944ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
