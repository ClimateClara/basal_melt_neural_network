{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f8f74-a672-4a47-9867-3c3f1309c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr 05 15:43 2022\n",
    "\n",
    "Script to train DNN on prepared input\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78591024-6443-4390-b166-fdb3091f29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from dask import delayed\n",
    "\n",
    "import distributed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6529b0-3d21-4c51-8268-2cf04ac7449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=8, dashboard_address=':8795', local_directory='/tmp', memory_limit='4GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0a826-5e46-4c4c-8f01-8c78f66bb74a",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d3c5b-bfb9-44d2-9fbd-4c7d98e91c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = ['OPM006', 'OPM016', 'OPM018', 'OPM031-2']\n",
    "TS_input = 'whole_prof' #'whole_prof' #'extrapolated'\n",
    "isf_list = [10, 11, 12, 13, 18, 21, 22, 23, 24, 25, 30, 31, 33, 38, 39, 40, 42, 43, 44, 45, 47, 48, 51, 52, 53, 54, 55, 58, 61, 65, 66, 69, 70, 71, 73, 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c555d4f-3013-4352-bcea-6f956f3aa058",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "outputpath_nn_models = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd269f-2a36-4405-bbc9-775fd86d7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_timetag = False\n",
    "if new_timetag:\n",
    "    datetag_dt = datetime.datetime.today()\n",
    "    timetag_dt = datetime.datetime.now()\n",
    "    timetag = str(datetag_dt.year)+str(datetag_dt.month).zfill(2)+str(datetag_dt.day).zfill(2)+'-'+str(timetag_dt.hour).zfill(2)+str(timetag_dt.minute).zfill(2)\n",
    "else:\n",
    "    #timetag = '20220422-1055'\n",
    "    timetag = '20220425-1050'\n",
    "\n",
    "new_path_model = outputpath_nn_models+timetag+'/'\n",
    "if not os.path.isdir(new_path_model):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in model folder so I created a new one, I hope that's ok!\")\n",
    "    os.mkdir(new_path_model)\n",
    "else:\n",
    "    print(\"This folder (\"+timetag+\") in model folder exists already!\")\n",
    "\n",
    "new_path_doc = outputpath_doc+timetag+'/'\n",
    "if not os.path.isdir(new_path_doc):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in doc folder so I created a new one, I hope that's ok!\")\n",
    "    os.mkdir(new_path_doc)\n",
    "else:\n",
    "    print(\"This folder (\"+timetag+\") in doc folder exists already!\")\n",
    "    \n",
    "new_path_input = inputpath_data+timetag+'/'\n",
    "if not os.path.isdir(new_path_input):\n",
    "    print(\"I did not find this folder (\"+timetag+\") in input folder so I created a new one, I hope that's ok!\")\n",
    "    os.mkdir(new_path_input)\n",
    "else:\n",
    "    print(\"This folder (\"+timetag+\") in input folder exists already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb866a81-32e9-4f25-89de-911f2bbe0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TS_input == 'whole_prof':\n",
    "    csv_path = inputpath_data + 'WHOLE_PROF/'\n",
    "elif TS_input == 'extrapolated':\n",
    "    csv_path = inputpath_data + 'EXTRAPOLATED_ISFDRAFT/'\n",
    "\n",
    "all_input_df = None        \n",
    "    \n",
    "for nemo_run in run_list:\n",
    "    \n",
    "    for kisf in tqdm(isf_list): \n",
    "    #for kisf in tqdm([23,66]): \n",
    "    #for kisf in tqdm([23]): \n",
    "        clean_df_nrun_kisf = pd.read_csv(csv_path + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+nemo_run+'.csv',index_col=[0,1,2])\n",
    "        clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "        clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "\n",
    "        if all_input_df is None:\n",
    "            all_input_df = clean_ds_nrun_kisf.copy()\n",
    "        else:\n",
    "            #print(clean_ds_nrun_kisf.index)\n",
    "            #print(all_input_df.index.max())\n",
    "            new_index = clean_ds_nrun_kisf.index.values + all_input_df.index.max().values+1\n",
    "            #print(new_index)\n",
    "            clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "            all_input_df = xr.concat([all_input_df, clean_ds_nrun_kisf], dim='index') \n",
    "\n",
    "all_input_df.to_netcdf(new_path_input + 'dataset_input_'+timetag+'.nc','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f8c3e-3340-4055-9a6e-9ddfd08486b2",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad373c-632d-4840-a32c-ec2ecc0d8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_df = xr.open_mfdataset(new_path_input + 'dataset_input_'+timetag+'.nc')#.load()#.chunk({'index': 10000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877aba2-7f76-49c4-aa75-f686944a02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 0.7\n",
    "\n",
    "all_indexes = all_input_df.index\n",
    "random_sample = np.random.choice(all_indexes, size=np.round(len(all_indexes)*0.7).astype(int), replace=False)\n",
    "\n",
    "data_train = all_input_df.sel(index=random_sample)\n",
    "data_test = all_input_df.drop_sel(index=random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd036822-d424-4600-85b5-0bbdb82a3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape_xr_ds(ds):\n",
    "    return (len(ds.index),len(ds.data_vars.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd8302-2759-40f8-b257-a7f36e9c8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_m_ice_per_y']\n",
    "x_train = data_train.drop_vars(['melt_m_ice_per_y'])\n",
    "\n",
    "y_test = data_test['melt_m_ice_per_y']\n",
    "x_test = data_test.drop_vars(['melt_m_ice_per_y'])\n",
    "\n",
    "print('Original data shape was : ', print_shape_xr_ds(all_input_df))\n",
    "print('x_train : ',print_shape_xr_ds(x_train), 'y_train : ',len(y_train))\n",
    "print('x_test  : ',print_shape_xr_ds(x_test),  'y_test  : ',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ddb85-1188-4090-baae-1747a4ea7fbd",
   "metadata": {},
   "source": [
    "DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398aa257-3916-4797-af33-ed2768c96894",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_method = 'interquart' #'std', 'interquart', 'minmax'\n",
    "\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "\n",
    "if norm_method == 'std':\n",
    "    x_range  = x_train.std()\n",
    "    y_range  = y_train.std()\n",
    "elif norm_method == 'interquart':\n",
    "    x_range  = x_train.quantile(0.9) - x_train.quantile(0.1)\n",
    "    y_range  = y_train.quantile(0.9) - y_train.quantile(0.1)\n",
    "elif norm_method == 'minmax':\n",
    "    x_range  = x_train.max() - x_train.min() \n",
    "    y_range  = y_train.max() - y_train.min() \n",
    "    \n",
    "x_train_norm = (x_train - x_mean)/x_range\n",
    "x_test_norm = (x_test - x_mean)/x_range\n",
    "\n",
    "y_train_norm = (y_train - y_mean)/y_range\n",
    "y_test_norm = (y_test - y_mean)/y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a6942-277e-4140-bd18-0419ab1b4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ds = xr.merge([x_mean,y_mean])\n",
    "mean_ds = mean_ds.assign_coords({'metric': 'mean_vars'})\n",
    "range_ds = xr.merge([x_range,y_range])\n",
    "range_ds = range_ds.assign_coords({'metric': 'std_vars'})\n",
    "summary_ds = xr.concat([mean_ds, range_ds], dim='metric')\n",
    "\n",
    "summary_ds.to_netcdf(new_path_model + 'dataset_norm_training_factors_'+timetag+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a4d80-660f-4a4f-ba2f-bcade4e61462",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_norm = xr.merge([x_train_norm,y_train_norm])\n",
    "data_train_norm.to_netcdf(new_path_model + 'dataset_norm_training_data_'+timetag+'.nc', 'w')\n",
    "data_test_norm = xr.merge([x_test_norm,y_test_norm])\n",
    "data_test_norm.to_netcdf(new_path_model + 'dataset_norm_test_data_'+timetag+'.nc', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9f28a-05a1-46cd-ab1e-25cb851aa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_path_doc+'info_'+timetag+'.log','w') as file:\n",
    "    file.write('Timetag: '+timetag+' \\n')\n",
    "    file.write('----- DATA ----- \\n')\n",
    "    file.write('Taraining data from: '+str(run_list)+'\\n')\n",
    "    file.write('Ice shelves: '+str(isf_list)+'\\n')\n",
    "    file.write('Input T and S format: '+TS_input+'\\n')\n",
    "    file.write('Norm method: '+norm_method+'\\n')\n",
    "    #file.write('Original data shape was : '+str(all_input_df.shape)+'\\n')\n",
    "    file.write('x_train : '+str(print_shape_xr_ds(x_train_norm))+', y_train : '+str(y_train_norm.values.shape)+'\\n')\n",
    "    file.write('x_test  : '+str(print_shape_xr_ds(x_test_norm))+', y_test  : '+str(y_test_norm.values.shape)+'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333d536-7f21-46e0-9754-4dc8d54f3bf6",
   "metadata": {},
   "source": [
    "BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fb3a4-ac0a-447b-b2d3-686c25d17ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_norm = xr.open_dataset(new_path_model + 'dataset_norm_training_data_'+timetag+'.nc')\n",
    "data_test_norm = xr.open_dataset(new_path_model + 'dataset_norm_test_data_'+timetag+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d217cb-abd1-410d-85d3-a1b2dcb0712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_norm = data_train_norm['melt_m_ice_per_y'].load()\n",
    "x_train_norm = data_train_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()\n",
    "\n",
    "y_test_norm = data_test_norm['melt_m_ice_per_y'].load()\n",
    "x_test_norm = data_test_norm.drop_vars(['melt_m_ice_per_y']).to_array().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f9bb2-0efb-473a-95f5-60966575b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_v2(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed41cc-af8d-45bd-9660-f3a1cfbbdb5c",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407a6f7-724f-480a-b114-6d91f17d530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train_norm.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7c944-375d-473f-821f-19d33ef51914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_v1( (input_size,) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc4905-b361-4450-b8f3-7a5d6337086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nb = 100\n",
    "batch_siz = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da646a40-1eeb-4518-8f3b-2869993e088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "    file.write('Input variables: '+','.join(map(str,x_train_norm['variable'].values))+'\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('----- MODEL ----- \\n')\n",
    "    with redirect_stdout(file):\n",
    "        model.summary()\n",
    "    file.write('\\n')\n",
    "    file.write('----- TRAINING ----- \\n')\n",
    "    file.write('Epochs: '+str(epoch_nb)+'\\n')\n",
    "    file.write('Batch size: '+str(batch_siz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cb558-58f3-4b1d-ba2a-8d84eef8e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                              patience=5, min_lr=0.001)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, min_lr=0.00000015, min_delta=0.007)\n",
    "\n",
    "time_start = time.time()\n",
    "history = model.fit(x_train_norm.T.values,\n",
    "                    y_train_norm.values,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_norm.T.values, y_test_norm.values),\n",
    "                   callbacks=[reduce_lr])\n",
    "time_end = time.time()\n",
    "timelength = time_end - time_start\n",
    "with open(new_path_doc+'info_'+timetag+'.log','a') as file:\n",
    "    file.write('\\n Training time (in s): '+str(timelength))\n",
    "model.save(new_path_model + 'model_nn_'+timetag+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea900440-fb6c-4a1a-b010-7c8daf559e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = new_path_model+'history_'+timetag+'.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c229628-e52c-496f-9eda-5dc22655b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### FROM THERE ON JUST TRYING AROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00b19b-d95f-4c2b-9d2b-cf7bb9bd41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2fc77-0832-4fe7-b306-9905c9fa2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4684ef3-7de7-4c82-861a-0efd6d970cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaea97c-5fb7-447f-8e35-206f893a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e2cb0-c8cb-4f3c-8520-7372a7da7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b812ef-a424-442a-9f98-9e28d9c8d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_norm = x_test_norm.to_array().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b21c8-33f8-4b6a-b189-4ea1387f3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_norm = y_test_norm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12053c2-bfac-4250-9422-8113367df9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_norm.T.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93f87b-284c-4720-8a96-8caa585d67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.apply_ufunc(model.fit,\n",
    "                x_train_norm.to_array().chunk({'index': 1000,'variable': 148}), #.expand_dims('other')\n",
    "                y_train_norm.chunk({'index': 1000}), #.expand_dims('other')\n",
    "                kwargs={'epochs': epoch_nb, \n",
    "                        'batch_size': batch_siz,\n",
    "                        'verbose': 1,\n",
    "                        'validation_data': (x_test_norm.T.values, y_test_norm.values), #.expand_dims('other')\n",
    "                        'callbacks': [reduce_lr]},\n",
    "                \n",
    "               input_core_dims=[[\"variable\", \"index\"],[\"index\", ]],\n",
    "               output_core_dims=[[\"index\" ,]],\n",
    "               dask='allowed')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748a4f1-7422-4136-abfe-24ae649a4fff",
   "metadata": {},
   "source": [
    "QUICK EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72954930-0364-4a3c-bcd4-dd5f1e820ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_norm.T.values, y_test_norm.values, verbose=1)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe132e-9f4e-4b14-865a-4ff8bcd35ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60582872-142d-4c61-86b0-820a74aa0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2839dd29-6ebf-4f72-8681-e3bdf49a755c",
   "metadata": {
    "tags": []
   },
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e1304-33b3-49d1-9450-b5a60a8421dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_df = None\n",
    "\n",
    "for nemo_run in run_list:\n",
    "    print(nemo_run)\n",
    "    \n",
    "    # read in the file\n",
    "    clean_df_nrun = pd.read_csv(new_path_model + 'dataframe_input_'+nemo_run+'.csv',index_col=[0,1,2])\n",
    "    clean_df_nrun.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # append the different runs\n",
    "    if all_input_df is None:\n",
    "        all_input_df = clean_df_nrun\n",
    "    else:\n",
    "        all_input_df = all_input_df.append(clean_df_nrun, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a3bdb-8197-4d5b-a98a-340bf3ea130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TS_input == 'whole_prof':\n",
    "    csv_path = inputpath_data + 'WHOLE_PROF/'\n",
    "elif TS_input == 'extrapolated':\n",
    "    csv_path = inputpath_data + 'EXTRAPOLATED_ISFDRAFT/'\n",
    "\n",
    "for nemo_run in run_list:\n",
    "    \n",
    "    for kisf in isf_list:\n",
    "        \n",
    "        # copy to folder for archive\n",
    "        src = csv_path + 'dataframe_input_'+str(kisf).zfill(3)+'_'+nemo_run+'.csv'\n",
    "        dst = new_path_input + 'dataframe_input_'+nemo_run+'.csv'\n",
    "        os.popen(f\"cp {src} {dst}\")\n",
    "    \n",
    "        print('copied '+TS_input+' input data from '+str(kisf)+' to '+nemo_run+' input folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e5c85-518b-40ef-88ae-cfebf516d506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
