{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr 05 15:43 2022\n",
    "\n",
    "Try using xarray input into the DNN directly\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_run = 'OPM016'\n",
    "inputpath_data='/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/NEMO_eORCA025.L121_'+nemo_run+'_ANT_STEREO/'\n",
    "inputpath_mask = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_profiles = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/T_S_PROF/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_plumes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/PLUMES/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_boxes = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/interim/BOXES/nemo_5km_'+nemo_run+'/'\n",
    "outputpath_melt = '/bettik/burgardc/SCRIPTS/basal_melt_param/data/processed/MELT_RATE/nemo_5km_'+nemo_run+'/'\n",
    "outputpath_nn = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/data/interim/'\n",
    "outputpath_doc = '/bettik/burgardc/SCRIPTS/basal_melt_neural_networks/custom_doc/'\n",
    "inputpath_tides = '/bettik/burgardc/DATA/BASAL_MELT_PARAM/interim/TIDES/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR EACH POINT:\n",
    "- T and S extrapolated to ice draft depth\n",
    "- Distance to front\n",
    "- Distance to the grounding line\n",
    "- x-slope ice draft\n",
    "- y-slope ice draft\n",
    "- x-slope bedrock\n",
    "- y-slope bedrock\n",
    "- Ice draft depth\n",
    "- Bathymetry\n",
    "- utide\n",
    "- Orientation of the cavity\n",
    "- Ice draft concentration\n",
    "- Mean bathymetry at entry (to add in future)\n",
    "- Max bathymetry (to add in future)\n",
    "- Target: melt m ice per yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_S_2D_isfdraft = xr.open_mfdataset(inputpath_profiles+'T_S_2D_fields_isf_draft.nc').sel(profile_domain=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dIF, dGL, longitude, latitude\n",
    "file_isf_orig = xr.open_dataset(inputpath_mask+'nemo_5km_isf_masks_and_info_and_distance_new.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=large_isf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T and S profiles\n",
    "file_TS_orig = xr.open_dataset(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_and_offshore_1980-2018.nc')\n",
    "file_TS = file_TS_orig.sel(Nisf=file_isf.Nisf)\n",
    "file_TS_dom = file_TS.sel(profile_domain=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_charac = xr.open_dataset(inputpath_plumes+'nemo_5km_plume_characteristics.nc')\n",
    "# Local slope\n",
    "local_ice_slope = plume_charac['alpha'].sel(option='appenB').drop('option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]\n",
    "file_mask_orig = xr.open_dataset(inputpath_data+'other_mask_vars_Ant_stereo.nc')\n",
    "file_mask_orig_cut = dfmt.cut_domain_stereo(file_mask_orig, map_lim, map_lim)\n",
    "file_other = xr.open_dataset(inputpath_data+'corrected_draft_bathy_isf.nc')#, chunks={'x': chunk_size, 'y': chunk_size})\n",
    "file_other_cut = dfmt.cut_domain_stereo(file_other, map_lim, map_lim)\n",
    "file_conc = xr.open_dataset(inputpath_data+'isfdraft_conc_Ant_stereo.nc')\n",
    "file_conc_cut = dfmt.cut_domain_stereo(file_conc, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathymetry, ice draft, concentration\n",
    "file_bed_orig = file_mask_orig_cut['bathy_metry']\n",
    "file_draft = file_other_cut['corrected_isfdraft'] \n",
    "file_isf_conc = file_conc_cut['isfdraft_conc']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_bedrock_slope = xr.open_dataset(inputpath_mask+'nemo_5km_bedrock_slope.nc')\n",
    "local_bedrock_slope = file_bedrock_slope['bedrock_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_slope = xr.open_dataset(inputpath_mask+'nemo_5km_slope_info_bedrock_draft.nc')\n",
    "file_orientation = xr.open_dataset(inputpath_mask+'nemo_5km_orientation_info.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utide_file = xr.open_dataset(inputpath_tides + 'tidal_velocity_nemo_Ant_stereo.nc')\n",
    "u_tide = dfmt.cut_domain_stereo(utide_file['ttv'], map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEMO_melt_rates_2D = xr.open_mfdataset(outputpath_melt+'melt_rates_2D_NEMO.nc')\n",
    "melt_rate = NEMO_melt_rates_2D['melt_m_ice_per_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all 2D data in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#, 'longitude', 'latitude'\n",
    "geometry_2D = file_isf[['dGL', 'dIF']].merge(file_draft).merge(file_bed_orig).merge(file_slope).merge(file_orientation).merge(u_tide)\n",
    "geometry_2D['dIF'] = geometry_2D['dIF'].where(np.isfinite(geometry_2D['dIF']), np.nan)\n",
    "time_dpdt_in = T_S_2D_isfdraft[['theta_in','salinity_in']].merge(melt_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D_br, time_dpdt_in_br = xr.broadcast(geometry_2D,time_dpdt_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input_xr = xr.merge([geometry_2D_br, time_dpdt_in_br]).transpose('y','x','time').drop('profile_domain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATAFRAME WITH ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = final_input_xr.drop('longitude').drop('latitude').to_dataframe()\n",
    "\n",
    "# remove rows where there are nans\n",
    "clean_df_yy = merged_df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index (time, x, y)\n",
    "clean_df_yy.reset_index(drop=True, inplace=True)\n",
    "clean_df_yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_timetag = True\n",
    "if new_timetag:\n",
    "    datetag_dt = datetime.datetime.today()\n",
    "    timetag_dt = datetime.datetime.now()\n",
    "    timetag = str(datetag_dt.year)+str(datetag_dt.month).zfill(2)+str(datetag_dt.day).zfill(2)+'-'+str(timetag_dt.hour).zfill(2)+str(timetag_dt.minute).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_yy.to_csv(outputpath_nn + 'dataframe_input_'+timetag+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDE INTO TRAIN AND TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(outputpath_nn + 'dataframe_input_'+timetag+'.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = clean_df.sample(frac=0.7, axis=0) \n",
    "data_test  = clean_df.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['melt_m_ice_per_y']\n",
    "x_train = data_train.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "y_test = data_test['melt_m_ice_per_y']\n",
    "x_test = data_test.drop(['melt_m_ice_per_y'], axis=1)\n",
    "\n",
    "print('Original data shape was : ',clean_df.shape)\n",
    "print('x_train : ',x_train.shape, 'y_train : ',y_train.shape)\n",
    "print('x_test  : ',x_test.shape,  'y_test  : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Data normalization\n",
    "**Note :** \n",
    " - All input data must be normalized, train and test.  \n",
    " - To do this we will **subtract the mean** and **divide by the standard deviation**.  \n",
    " - But test data should not be used in any way, even for normalization.  \n",
    " - The mean and the standard deviation will therefore only be calculated with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.mean()/x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_method = 'interquart' #'std', 'interquart', 'minmax'\n",
    "\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "\n",
    "if norm_method == 'std':\n",
    "    x_range  = x_train.std()\n",
    "    y_range  = y_train.std()\n",
    "elif norm_method == 'interquart':\n",
    "    x_range  = x_train.quantile(0.9) - x_train.quantile(0.1)\n",
    "    y_range  = y_train.quantile(0.9) - y_train.quantile(0.1)\n",
    "elif norm_method == 'minmax':\n",
    "    x_range  = x_train.max() - x_train.min() \n",
    "    y_range  = y_train.max() - y_train.min() \n",
    "    \n",
    "x_train_norm = (x_train - x_mean)/x_range\n",
    "x_test_norm = (x_test - x_mean)/x_range\n",
    "\n",
    "y_train_norm = (y_train - y_mean)/y_range\n",
    "y_test_norm = (y_test - y_mean)/y_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "summary_df['x_mean'] = x_mean\n",
    "summary_df['x_range'] = x_range\n",
    "summary_df = summary_df.T \n",
    "summary_df['melt_m_ice_per_y'] = [y_mean, y_range]\n",
    "summary_df.to_csv(outputpath_nn + 'dataframe_norm_training_data_'+timetag+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "#display(x_train.head(5).style.format(\"{0:.2f}\").set_caption(\"Few lines of the dataset :\"))\n",
    "\n",
    "x_train_arr, y_train_arr = np.array(x_train_norm), np.array(y_train_norm)\n",
    "x_test_arr,  y_test_arr  = np.array(x_test_norm),  np.array(y_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "def get_model_v2(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(1, name='Output'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "#def get_model_v1(shape):\n",
    "#    nodes = 256\n",
    "#    #activ = 'sigmoid'\n",
    "#   activ = 'relu'   # standard\n",
    "#    #activ = 'tanh'\n",
    "#    #activ = 'selu'\n",
    "#    model = keras.models.Sequential()\n",
    "#    model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n1'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n2'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n3'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n4'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n5'))\n",
    "#    model.add(keras.layers.Dense(nodes, activation= activ, name='Dense_n6'))\n",
    "#    model.add(keras.layers.Dense(1, name='Output'))  \n",
    "#                                    # nbvect = number of elements of target vector\n",
    "#\n",
    "#    model.compile(optimizer = 'rmsprop',\n",
    "#                  loss      = 'mse',                  # mse, mean quadratic error \n",
    "#              metrics   = ['mae', 'mse'] )        # mae =  mean absolute error\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x_train_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_v1( (input_size,) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_nb = 50\n",
    "batch_siz = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputpath_doc+timetag+'.log','w') as file:\n",
    "    file.write('Timetag: '+timetag+' \\n')\n",
    "    file.write('----- DATA ----- \\n')\n",
    "    file.write('Training data from: '+nemo_run+'\\n')\n",
    "    file.write('Norm method: '+norm_method+'\\n')\n",
    "    file.write('Original data shape was : '+str(clean_df.shape)+'\\n')\n",
    "    file.write('x_train : '+str(x_train.shape)+', y_train : '+str(y_train.shape)+'\\n')\n",
    "    file.write('x_test  : '+str(x_test.shape)+', y_test  : '+str(y_test.shape)+'\\n') \n",
    "    file.write('Input variables: '+','.join(map(str,x_train_norm.columns))+'\\n')\n",
    "    file.write('\\n')\n",
    "    file.write('----- MODEL ----- \\n')\n",
    "    with redirect_stdout(file):\n",
    "        model.summary()\n",
    "    file.write('\\n')\n",
    "    file.write('----- TRAINING ----- \\n')\n",
    "    file.write('Epochs: '+str(epoch_nb)+'\\n')\n",
    "    file.write('Batch size: '+str(batch_siz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "history = model.fit(x_train_arr,\n",
    "                    y_train_arr,\n",
    "                    epochs          = epoch_nb,\n",
    "                    batch_size      = batch_siz,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test_arr, y_test_arr))\n",
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelength = time_end - time_start\n",
    "with open(outputpath_doc+timetag+'.log','a') as file:\n",
    "    file.write('\\n Training time (in s): '+str(timelength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(outputpath_nn + 'model_nn_'+timetag+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_arr, y_test_arr, verbose=1)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag.plot_history(history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                                'MAE' :['mae', 'val_mae'],\n",
    "                                'LOSS':['loss','val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kk, yy in enumerate(tqdm(range(2007,2008))):\n",
    "\n",
    "    # take profile of T and S for that year\n",
    "    TS_isf_tt = time_dpdt_in.sel(time=yy).drop('time')\n",
    "    # T and S profiles to dataframe\n",
    "    TS_isf_df = TS_isf_tt.drop('longitude').drop('latitude').to_dataframe()\n",
    "\n",
    "    # Geometry info to dataframe\n",
    "    length_df = len(geometry_2D.x)*len(geometry_2D.y)\n",
    "    geo_df = geometry_2D.drop('longitude').drop('latitude').to_dataframe()\n",
    "\n",
    "    # write out melt rate for that year (target)\n",
    "    melt_rate_isf_tt = melt_rate.sel(time=yy)\n",
    "    # transform to dataframe\n",
    "    melt_df = melt_rate_isf_tt.to_dataframe()#.drop(['mapping'],axis=1)#.reset_index()\n",
    "    melt_df = melt_df.drop(['time'],axis=1)\n",
    "\n",
    "    # merge properties and melt\n",
    "    merged_df0 = pd.merge(geo_df,melt_df,how='left',on=['x','y'])\n",
    "    merged_df_val = pd.merge(TS_isf_df,merged_df0,how='left',on=['x','y'])\n",
    "\n",
    "    # remove rows where there are nans\n",
    "    clean_df_yy_val = merged_df_val.dropna()\n",
    "    # remove the x and y axis\n",
    "    # clean_df = clean_df.drop(['x'], axis=1).drop(['y'], axis=1)\n",
    "    if kk > 0:\n",
    "        merged_yy_df_val = merged_yy_df_val.append(clean_df_yy, ignore_index = True)\n",
    "    else:\n",
    "        merged_yy_df_val = clean_df_yy_val.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_val = merged_yy_df_val.reset_index().drop(['x'], axis=1).drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = clean_df_val['melt_m_ice_per_y']\n",
    "x_val = clean_df_val.drop(['melt_m_ice_per_y'], axis=1).drop('profile_domain', 1)\n",
    "\n",
    "x_val_norm = x_val.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','theta_in','salinity_in']:\n",
    "    mean = x_train[ccol].mean()\n",
    "    std  = x_train[ccol].std()\n",
    "    x_val_norm[ccol] = (x_val[ccol] - mean) / std\n",
    "\n",
    "x_val_arr, y_val_arr = np.array(x_val_norm), np.array(y_val)\n",
    "\n",
    "#my_data=np.array(x_val_arr)#.reshape(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(x_val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_pd_s = pd.Series(y_out[:,0],index=merged_yy_df_val.index,name='computed_melt') \n",
    "y_target_pd_s = pd.Series(y_val_arr,index=merged_yy_df_val.index,name='reference_melt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_xr = y_out_pd_s.to_xarray()\n",
    "y_target_xr = y_target_pd_s.to_xarray()\n",
    "y_to_compare = xr.merge([y_out_xr.T, y_target_xr.T]).sortby('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(0,80)\n",
    "plt.figure()\n",
    "plt.scatter(y_to_compare['computed_melt'].values.flatten(),y_to_compare['reference_melt'].values.flatten(), s=10, edgecolors='None',alpha=0.2)\n",
    "plt.plot(xx,xx,'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_melt = y_to_compare['computed_melt']\n",
    "ref_melt = y_to_compare['reference_melt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_m = min(computed_melt.min(), ref_melt.min())\n",
    "max_m = max(computed_melt.max(), ref_melt.max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "\n",
    "f = plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "computed_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax1.set_title('Neural Network [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "ref_melt.plot(ax=ax2, vmin=minlim,vmax=lim, cmap=cmap)\n",
    "ax2.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(computed_melt - ref_melt).plot(ax=ax3)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('NN - Ref [m ice/y]')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_compare.to_netcdf(outputpath_nn+'prediction_linreg_2007_allisf.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((y_out - y_val_arr)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )\n",
    "print(\"Prediction : {:.2f} m ice per y\".format(predictions[0][0]))\n",
    "print(\"Reality    : {:.2f} m ice per y\".format(y_val_arr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARE WITH SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisf = 44\n",
    "mask_isf = file_isf['ISF_mask']==kisf\n",
    "if kisf == 11 or kisf == 21:\n",
    "    mask_isf = (file_isf['ISF_mask']==11) | (file_isf['ISF_mask']==21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_param = xr.open_dataset(outputpath_melt+'melt_rates_2D_quadratic_local_tuned_correctedTS.nc')\n",
    "melt_simple = simple_param['melt_m_ice_per_y'].sel(profile_domain=50,time=2007).where(mask_isf, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_compare_nn = xr.open_dataset(outputpath_nn+'prediction_linreg_2007_allisf.nc')\n",
    "computed_melt = y_to_compare_nn['computed_melt'].where(mask_isf, drop=True)\n",
    "ref_melt = y_to_compare_nn['reference_melt'].where(mask_isf, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_melt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/bettik/burgardc/PLOTS/generic_plots/'\n",
    "min_m = min(computed_melt.min(),ref_melt.min())\n",
    "max_m = max(computed_melt.max(),ref_melt.max())\n",
    "#min_m = min((computed_melt - ref_melt).min(),(melt_simple - ref_melt).min())\n",
    "#max_m = max((computed_melt - ref_melt).max(),(melt_simple - ref_melt).max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8.24*1.3, 8.24/3)\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ref_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=mpl.cm.coolwarm)\n",
    "ax1.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "#(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=-5,vmax=5, cmap=mpl.cm.BrBG_r)\n",
    "ax2.set_title('Neural Network param - Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "(melt_simple - ref_melt).plot(ax=ax3, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "#(melt_simple - ref_melt).plot(ax=ax3, vmin=-5,vmax=5, cmap=mpl.cm.BrBG_r)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('Simple physical param - Reference [m ice/y]')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(plot_path+'comparison_proof_of_concept_nn_antarctica_isf'+str(kisf).zfill(3)+'.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/bettik/burgardc/PLOTS/generic_plots/'\n",
    "min_m = min(computed_melt.min(),ref_melt.min())\n",
    "max_m = max(computed_melt.max(),ref_melt.max())\n",
    "#min_m = min((computed_melt - ref_melt).min(),(melt_simple - ref_melt).min())\n",
    "#max_m = max((computed_melt - ref_melt).max(),(melt_simple - ref_melt).max())\n",
    "lim = max(abs(min_m),abs(max_m))\n",
    "\n",
    "if min_m < 0:\n",
    "    cmap = mpl.cm.coolwarm\n",
    "    minlim = -lim\n",
    "else:\n",
    "    cmap = mpl.cm.viridis\n",
    "    minlim = 0\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8.24*1.3, 8.24/3)\n",
    "\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ref_melt.plot(ax=ax1, vmin=minlim,vmax=lim, cmap=mpl.cm.coolwarm)\n",
    "ax1.set_title('Reference [m ice/y]')\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2, sharex = ax1, sharey = ax1)\n",
    "#(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "(y_to_compare_nn['computed_melt'] - ref_melt).plot(ax=ax2, vmin=-5,vmax=5, cmap=mpl.cm.BrBG_r)\n",
    "ax2.set_title('Neural Network param - Reference [m ice/y]')\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3, sharex = ax1, sharey = ax1)\n",
    "#(melt_simple - ref_melt).plot(ax=ax3, vmin=minlim,vmax=lim, cmap=mpl.cm.BrBG_r)\n",
    "(melt_simple - ref_melt).plot(ax=ax3, vmin=-5,vmax=5, cmap=mpl.cm.BrBG_r)\n",
    "ax3.set_xticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax3.set_title('Simple physical param - Reference [m ice/y]')\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(plot_path+'comparison_proof_of_concept_nn_antarctica_isf'+str(kisf).zfill(3)+'_minmax5.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.axes(projection=ccrs.SouthPolarStereo(central_longitude=0,true_scale_latitude=-71))\n",
    "#ax.coastlines(resolution='50m', linewidth=0.5)\n",
    "#ax.pcolormesh(melt_simple.longitude,melt_simple.latitude,melt_simple,transform=ccrs.PlateCarree(),rasterized=True)\n",
    "#ax.set_extent([-180, 180, -90, -60], crs=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======== TO KEEP FOR THE FUTURE ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column - for normalization with min and max\n",
    "\n",
    "normalized_clean_df = clean_df.copy()\n",
    "\n",
    "for ccol in ['dGL','dIF','alpha','bedrock_slope','corrected_isfdraft','bathy_metry','longitude','latitude','melt_cavity','time']:\n",
    "    max_ccol = clean_df[ccol].max()\n",
    "    min_ccol = clean_df[ccol].min()\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_ccol)/(max_ccol - min_ccol)\n",
    "\n",
    "max_T = clean_df[T_list].max().max()\n",
    "min_T = clean_df[T_list].min().min()\n",
    "max_S = clean_df[S_list].max().max()\n",
    "min_S = clean_df[S_list].min().min()\n",
    "\n",
    "for ccol in [T_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_T)/(max_T - min_T)\n",
    "\n",
    "for ccol in [S_list]:\n",
    "    normalized_clean_df[ccol] = (clean_df[ccol] - min_S)/(max_S - min_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
